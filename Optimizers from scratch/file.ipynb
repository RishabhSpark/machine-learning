{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uRytT_zEXkKr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Class Adam Optimizer\n",
        "class AdamOptim():\n",
        "    def __init__(self, eta=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.m_dw, self.v_dw = 0, 0\n",
        "        self.m_db, self.v_db = 0, 0\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.eta = eta\n",
        "    def update(self, t, w, b, dw, db):\n",
        "        # *** weights *** #\n",
        "        self.m_dw = self.beta1*self.m_dw + (1-self.beta1)*dw\n",
        "        # *** biases *** #\n",
        "        self.m_db = self.beta1*self.m_db + (1-self.beta1)*db\n",
        "\n",
        "        # *** weights *** #\n",
        "        self.v_dw = self.beta2*self.v_dw + (1-self.beta2)*(dw**2)\n",
        "        # *** biases *** #\n",
        "        self.v_db = self.beta2*self.v_db + (1-self.beta2)*(db)\n",
        "\n",
        "        ## bias correction\n",
        "        m_dw_corr = self.m_dw/(1-self.beta1)\n",
        "        m_db_corr = self.m_db/(1-self.beta1)\n",
        "        v_dw_corr = self.v_dw/(1-self.beta2)\n",
        "        v_db_corr = self.v_db/(1-self.beta2)\n",
        "\n",
        "        ## update weights and biases\n",
        "        w = w - self.eta*(m_dw_corr/(np.sqrt(v_dw_corr)+self.epsilon))\n",
        "        b = b - self.eta*(m_db_corr/(np.sqrt(v_db_corr)+self.epsilon))\n",
        "        return w, b"
      ],
      "metadata": {
        "id": "3Xc9zpYxXm5r"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(m):\n",
        "    return m**2-2*m+1\n",
        "\n",
        "## take derivative\n",
        "def grad_function(m):\n",
        "    return 2*m-2\n",
        "\n",
        "def check_convergence(w0, w1):\n",
        "    return (w0 == w1)"
      ],
      "metadata": {
        "id": "ujPCJ_eZkmkO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize w_0 and b_0 with random values using function\n",
        "w_0 = random.random()\n",
        "b_0 = random.random()\n",
        "adam = AdamOptim()\n",
        "# Initialize iteration with 1\n",
        "t = 1\n",
        "converged = False\n",
        "x = []\n",
        "l = []\n",
        "while not converged:\n",
        "    dw = grad_function(w_0)\n",
        "    db = grad_function(b_0)\n",
        "    loss = loss_function(w_0)\n",
        "    w_0_old = w_0\n",
        "    w_0, b_0 = adam.update(t,w=w_0, b=b_0, dw=dw, db=db)\n",
        "    if check_convergence(w_0, w_0_old):\n",
        "        print('converged after '+str(t)+' iterations')\n",
        "        break\n",
        "    else:\n",
        "        print('iteration '+str(t)+': weight='+str(w_0)+':loss='+str(loss))\n",
        "        l.append(loss)\n",
        "        x.append(t)\n",
        "        t+=1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3KQoeraknNj",
        "outputId": "4fecd09f-c2f5-4d70-9d2b-e12474929678"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1: weight=0.1659443771679522:loss=0.7124298943343961\n",
            "iteration 2: weight=0.17937835273514294:loss=0.6956487819777551\n",
            "iteration 3: weight=0.1950160000781907:loss=0.6734198879596875\n",
            "iteration 4: weight=0.21218563248517458:loss=0.6479992401301155\n",
            "iteration 5: weight=0.2304460018127515:loss=0.6206514776627844\n",
            "iteration 6: weight=0.24948069963824404:loss=0.5922133561259797\n",
            "iteration 7: weight=0.26905109351021916:loss=0.5632792202154997\n",
            "iteration 8: weight=0.2889713356525734:loss=0.5342863038986063\n",
            "iteration 9: weight=0.3090935229266421:loss=0.5055617615236855\n",
            "iteration 10: weight=0.32929817101485476:loss=0.4773517600619184\n",
            "iteration 11: weight=0.3494877390611979:loss=0.44984094340401903\n",
            "iteration 12: weight=0.36958203069119155:loss=0.42316620163171215\n",
            "iteration 13: weight=0.3895148131121504:loss=0.3974268160274418\n",
            "iteration 14: weight=0.4092312618141587:loss=0.3726921634094926\n",
            "iteration 15: weight=0.42868598495206106:loss=0.34900770201769116\n",
            "iteration 16: weight=0.4478414668589918:loss=0.32639970379019656\n",
            "iteration 17: weight=0.4666668221941794:loss=0.3048790457204299\n",
            "iteration 18: weight=0.4851367852150613:loss=0.2844442785484551\n",
            "iteration 19: weight=0.5032308802726015:loss=0.2650841299386819\n",
            "iteration 20: weight=0.520932734196102:loss=0.2467795583147343\n",
            "iteration 21: weight=0.5382295013041614:loss=0.22950544516482263\n",
            "iteration 22: weight=0.5551113788961884:loss=0.21323199346580357\n",
            "iteration 23: weight=0.5715711962099488:loss=0.19792588518765086\n",
            "iteration 24: weight=0.5876040635954186:loss=0.1835512399169742\n",
            "iteration 25: weight=0.6032070714617686:loss=0.17007040836301157\n",
            "iteration 26: weight=0.6183790306767671:loss=0.15744462813794602\n",
            "iteration 27: weight=0.6331202477240802:loss=0.1456345642272039\n",
            "iteration 28: weight=0.6474323291847597:loss=0.13460075263004034\n",
            "iteration 29: weight=0.6613180110974132:loss=0.1243039625040836\n",
            "iteration 30: weight=0.6747810095337725:loss=0.11470548960701188\n",
            "iteration 31: weight=0.6878258893512308:loss=0.10576739175987215\n",
            "iteration 32: weight=0.7004579485872118:loss=0.09745267535934998\n",
            "iteration 33: weight=0.7126831163687348:loss=0.08972544056458154\n",
            "iteration 34: weight=0.7245078625444191:loss=0.08255099161958201\n",
            "iteration 35: weight=0.7359391175208173:loss=0.07589591779984461\n",
            "iteration 36: weight=0.7469842010122135:loss=0.06972814965568475\n",
            "iteration 37: weight=0.7576507586020546:loss=0.06401699453742793\n",
            "iteration 38: weight=0.7679467051721919:loss=0.05873315480615959\n",
            "iteration 39: weight=0.777880174388779:loss=0.05384873164044168\n",
            "iteration 40: weight=0.7874594735454987:loss=0.04933721692955928\n",
            "iteration 41: weight=0.7966930431594065:loss=0.045173475385556605\n",
            "iteration 42: weight=0.8055894207949797:loss=0.04133371869978297\n",
            "iteration 43: weight=0.8141572086603294:loss=0.03779547330683142\n",
            "iteration 44: weight=0.8224050445778782:loss=0.03453754309292034\n",
            "iteration 45: weight=0.8303415759817445:loss=0.03153996819138538\n",
            "iteration 46: weight=0.8379754366368825:loss=0.02878398084035816\n",
            "iteration 47: weight=0.8453152258118225:loss=0.02625195913300893\n",
            "iteration 48: weight=0.8523694896685275:loss=0.023927379365647483\n",
            "iteration 49: weight=0.8591467046601975:loss=0.021794767580730956\n",
            "iteration 50: weight=0.8656552627514548:loss=0.019839650808081588\n",
            "iteration 51: weight=0.8719034582957675:loss=0.018048508426380616\n",
            "iteration 52: weight=0.8778994764226764:loss=0.016408723996584218\n",
            "iteration 53: weight=0.8836513828027687:loss=0.014908537857856508\n",
            "iteration 54: weight=0.8891671146717312:loss=0.013537000723707893\n",
            "iteration 55: weight=0.8944544730064841:loss=0.01228392847018922\n",
            "iteration 56: weight=0.8995211157566022:loss=0.011139858268338965\n",
            "iteration 57: weight=0.9043745520431715:loss=0.010096006178798134\n",
            "iteration 58: weight=0.9090221372450858:loss=0.009144226296944113\n",
            "iteration 59: weight=0.9134710688997152:loss=0.008276971511452014\n",
            "iteration 60: weight=0.9177283833510015:loss=0.007487255917357882\n",
            "iteration 61: weight=0.9218009530834756:loss=0.006768618906039747\n",
            "iteration 62: weight=0.9256954846855339:loss=0.006115090938652812\n",
            "iteration 63: weight=0.929418517389646:loss=0.005521160996117769\n",
            "iteration 64: weight=0.9329764221410641:loss=0.004981745687475669\n",
            "iteration 65: weight=0.9363754011501174:loss=0.004492159989012867\n",
            "iteration 66: weight=0.9396214878863639:loss=0.004048089578808534\n",
            "iteration 67: weight=0.9427205474757803:loss=0.003645564725056505\n",
            "iteration 68: weight=0.9456782774648228:loss=0.0032809356814743618\n",
            "iteration 69: weight=0.9485002089176402:loss=0.0029508495391887823\n",
            "iteration 70: weight=0.9511917078149708:loss=0.002652228481526686\n",
            "iteration 71: weight=0.9537579767253478:loss=0.002382249386019164\n",
            "iteration 72: weight=0.9562040567211793:loss=0.002138324716533524\n",
            "iteration 73: weight=0.9585348295140815:loss=0.0019180846476817281\n",
            "iteration 74: weight=0.9607550197855439:loss=0.0017193603634262367\n",
            "iteration 75: weight=0.9628691976905938:loss=0.0015401684720330877\n",
            "iteration 76: weight=0.9648817815136296:loss=0.001378696480140218\n",
            "iteration 77: weight=0.9667970404569995:loss=0.00123328926965649\n",
            "iteration 78: weight=0.9686190975442369:loss=0.001102436522414152\n",
            "iteration 79: weight=0.970351932621119:loss=0.0009847610389380757\n",
            "iteration 80: weight=0.971999385438908:loss=0.0008790078993026196\n",
            "iteration 81: weight=0.9735651588052567:loss=0.0007840344157988843\n",
            "iteration 82: weight=0.9750528217893264:loss=0.0006988008289913195\n",
            "iteration 83: weight=0.9764658129686724:loss=0.0006223617006750892\n",
            "iteration 84: weight=0.9778074437064085:loss=0.0005538579592254678\n",
            "iteration 85: weight=0.9790809014480596:loss=0.000492509554844256\n",
            "iteration 86: weight=0.9802892530283698:loss=0.0004376086842258209\n",
            "iteration 87: weight=0.9814354479791347:loss=0.0003885135461796363\n",
            "iteration 88: weight=0.9825223218298924:loss=0.000344642591735389\n",
            "iteration 89: weight=0.9835525993940195:loss=0.00030546923421781447\n",
            "iteration 90: weight=0.984528898033462:loss=0.0002705169866935586\n",
            "iteration 91: weight=0.9854537308959622:loss=0.00023935499605898247\n",
            "iteration 92: weight=0.9863295101192435:loss=0.00021159394484704386\n",
            "iteration 93: weight=0.9871585499971778:loss=0.00018688229357988195\n",
            "iteration 94: weight=0.9879430701034868:loss=0.00016490283817494067\n",
            "iteration 95: weight=0.9886851983690225:loss=0.0001453695585293957\n",
            "iteration 96: weight=0.9893869741091328:loss=0.00012802473594841768\n",
            "iteration 97: weight=0.9900503509980524:loss=0.00011263631856017753\n",
            "iteration 98: weight=0.9906771999876575:loss=9.899551526193484e-05\n",
            "iteration 99: weight=0.9912693121682988:loss=8.691460007015372e-05\n",
            "iteration 100: weight=0.9918284015697764:loss=7.622491001457732e-05\n",
            "iteration 101: weight=0.9923561079008375:loss=6.677502090479681e-05\n",
            "iteration 102: weight=0.9928539992258836:loss=5.8429086423616994e-05\n",
            "iteration 103: weight=0.9933235745778424:loss=5.106532706367073e-05\n",
            "iteration 104: weight=0.9937662665064189:loss=4.4574656417606384e-05\n",
            "iteration 105: weight=0.9941834435611708:loss=3.885943326897845e-05\n",
            "iteration 106: weight=0.9945764127090707:loss=3.383232880604936e-05\n",
            "iteration 107: weight=0.9949464216864103:loss=2.9415299102275583e-05\n",
            "iteration 108: weight=0.9952946612850855:loss=2.553865377152853e-05\n",
            "iteration 109: weight=0.9956222675734605:loss=2.214021242208375e-05\n",
            "iteration 110: weight=0.9959303240521604:loss=1.9164541198324336e-05\n",
            "iteration 111: weight=0.9962198637452728:loss=1.656226232038538e-05\n",
            "iteration 112: weight=0.9964918712275619:loss=1.4289430104286183e-05\n",
            "iteration 113: weight=0.9967472845884052:loss=1.2306967483999287e-05\n",
            "iteration 114: weight=0.996986997333261:loss=1.0580157548845115e-05\n",
            "iteration 115: weight=0.997211860223559:loss=9.078185069788525e-06\n",
            "iteration 116: weight=0.9974226830559827:loss=7.773723412984879e-06\n",
            "iteration 117: weight=0.9976202363821793:loss=6.642562629943782e-06\n",
            "iteration 118: weight=0.9978052531699895:loss=5.66327487672158e-06\n",
            "iteration 119: weight=0.9979784304073381:loss=4.816913647842647e-06\n",
            "iteration 120: weight=0.9981404306499719:loss=4.086743617959243e-06\n",
            "iteration 121: weight=0.9982918835142613:loss=3.4579981675886273e-06\n",
            "iteration 122: weight=0.9984333871163155:loss=2.917661928836246e-06\n",
            "iteration 123: weight=0.9985655094586809:loss=2.4542759273771253e-06\n",
            "iteration 124: weight=0.9986887897659125:loss=2.057763113127997e-06\n",
            "iteration 125: weight=0.9988037397703153:loss=1.7192722779890701e-06\n",
            "iteration 126: weight=0.99891084494917:loss=1.4310385371141265e-06\n",
            "iteration 127: weight=0.9990105657147501:loss=1.1862587246946887e-06\n",
            "iteration 128: weight=0.9991033385584465:loss=9.789802047910356e-07\n",
            "iteration 129: weight=0.9991895771503074:loss=8.040017407395794e-07\n",
            "iteration 130: weight=0.9992696733952978:loss=6.567851953409587e-07\n",
            "iteration 131: weight=0.9993439984475703:loss=5.333769494964002e-07\n",
            "iteration 132: weight=0.9994129036840311:loss=4.303380367609577e-07\n",
            "iteration 133: weight=0.9994767216384696:loss=3.4468208420790347e-07\n",
            "iteration 134: weight=0.9995357668975015:loss=2.738202437013726e-07\n",
            "iteration 135: weight=0.9995903369595628:loss=2.15512373502591e-07\n",
            "iteration 136: weight=0.9996407130581687:loss=1.678238067404081e-07\n",
            "iteration 137: weight=0.9996871609506317:loss=1.2908710655956668e-07\n",
            "iteration 138: weight=0.9997299316734127:loss=9.786827082081118e-08\n",
            "iteration 139: weight=0.9997692622652553:loss=7.293690107434969e-08\n",
            "iteration 140: weight=0.9998053764592286:loss=5.3239902264579086e-08\n",
            "iteration 141: weight=0.9998384853447827:loss=3.787832258961288e-08\n",
            "iteration 142: weight=0.9998687880008916:loss=2.608698379535923e-08\n",
            "iteration 143: weight=0.9998964721013373:loss=1.7216588710233793e-08\n",
            "iteration 144: weight=0.9999217144931596:loss=1.0718025800215969e-08\n",
            "iteration 145: weight=0.9999446817492725:loss=6.128620611001168e-09\n",
            "iteration 146: weight=0.9999655306962203:loss=3.0601088329973436e-09\n",
            "iteration 147: weight=0.9999844089180202:loss=1.188132925378227e-09\n",
            "iteration 148: weight=1.0000014552370142:loss=2.43081887951746e-10\n",
            "iteration 149: weight=1.0000168001726237:loss=2.1176393971700236e-12\n",
            "iteration 150: weight=1.0000305663788753:loss=2.8224578230151565e-10\n",
            "iteration 151: weight=1.000042869061542:loss=9.34303523436597e-10\n",
            "iteration 152: weight=1.000053816375716:loss=1.8377563915095152e-09\n",
            "iteration 153: weight=1.0000635098046051:loss=2.896202389024438e-09\n",
            "iteration 154: weight=1.0000720445203186:loss=4.033495315525215e-09\n",
            "iteration 155: weight=1.0000795097273854:loss=5.190412988298476e-09\n",
            "iteration 156: weight=1.0000859889897218:loss=6.321796641728383e-09\n",
            "iteration 157: weight=1.0000915605417424:loss=7.3941064382410104e-09\n",
            "iteration 158: weight=1.0000962975842835:loss=8.383332694705814e-09\n",
            "iteration 159: weight=1.000100268565987:loss=9.273224632622146e-09\n",
            "iteration 160: weight=1.0001035374507685:loss=1.005378535978707e-08\n",
            "iteration 161: weight=1.0001061639719733:loss=1.0720003773556641e-08\n",
            "iteration 162: weight=1.000108203873798:loss=1.127078896878686e-08\n",
            "iteration 163: weight=1.0001097091405413:loss=1.1708078284655699e-08\n",
            "iteration 164: weight=1.00011072821422:loss=1.203609545363804e-08\n",
            "iteration 165: weight=1.0001113062010711:loss=1.226073731430688e-08\n",
            "iteration 166: weight=1.0001114850674375:loss=1.2389070436213956e-08\n",
            "iteration 167: weight=1.0001113038255212:loss=1.2428920337370641e-08\n",
            "iteration 168: weight=1.000110798709462:loss=1.2388541525965024e-08\n",
            "iteration 169: weight=1.0001100033421888:loss=1.2276353933415862e-08\n",
            "iteration 170: weight=1.0001089488934691:loss=1.2100735302666976e-08\n",
            "iteration 171: weight=1.0001076642295663:loss=1.1869861316071706e-08\n",
            "iteration 172: weight=1.0001061760548984:loss=1.1591586357440065e-08\n",
            "iteration 173: weight=1.0001045090460758:loss=1.1273354694196769e-08\n",
            "iteration 174: weight=1.0001026859786804:loss=1.0922140747382514e-08\n",
            "iteration 175: weight=1.000100727847132:loss=1.0544410233848112e-08\n",
            "iteration 176: weight=1.0000986539779777:loss=1.0146099294061628e-08\n",
            "iteration 177: weight=1.000096482136918:loss=9.732607386681025e-09\n",
            "iteration 178: weight=1.000094228629883:loss=9.308802839669283e-09\n",
            "iteration 179: weight=1.0000919083984423:loss=8.879034618658466e-09\n",
            "iteration 180: weight=1.000089535109835:loss=8.447153643231786e-09\n",
            "iteration 181: weight=1.0000871212418847:loss=8.016535879562525e-09\n",
            "iteration 182: weight=1.000084678163058:loss=7.590110762123459e-09\n",
            "iteration 183: weight=1.0000822162079068:loss=7.170391391753128e-09\n",
            "iteration 184: weight=1.0000797447481344:loss=6.75950473372211e-09\n",
            "iteration 185: weight=1.0000772722595046:loss=6.3592249244237564e-09\n",
            "iteration 186: weight=1.00007480638481:loss=5.971002137172832e-09\n",
            "iteration 187: weight=1.000072353993102:loss=5.59599522276244e-09\n",
            "iteration 188: weight=1.0000699212353785:loss=5.2351003532180584e-09\n",
            "iteration 189: weight=1.0000675135969135:loss=4.888979221462364e-09\n",
            "iteration 190: weight=1.0000651359464066:loss=4.558085686667823e-09\n",
            "iteration 191: weight=1.000062792582123:loss=4.2426915314308644e-09\n",
            "iteration 192: weight=1.0000604872751817:loss=3.942908444187765e-09\n",
            "iteration 193: weight=1.0000582233101478:loss=3.6587104457197483e-09\n",
            "iteration 194: weight=1.0000560035230748:loss=3.3899538731674284e-09\n",
            "iteration 195: weight=1.0000538303371351:loss=3.1363946995099923e-09\n",
            "iteration 196: weight=1.0000517057959726:loss=2.8977051869105708e-09\n",
            "iteration 197: weight=1.0000496315949017:loss=2.6734894298385825e-09\n",
            "iteration 198: weight=1.0000476091100747:loss=2.463295123433795e-09\n",
            "iteration 199: weight=1.0000456394257295:loss=2.266627330271831e-09\n",
            "iteration 200: weight=1.0000437233596287:loss=2.0829571401037583e-09\n",
            "iteration 201: weight=1.0000418614867892:loss=1.911732105952524e-09\n",
            "iteration 202: weight=1.0000400541616052:loss=1.7523840156741244e-09\n",
            "iteration 203: weight=1.000038301538453:loss=1.6043357753403598e-09\n",
            "iteration 204: weight=1.0000366035908683:loss=1.4670078485323756e-09\n",
            "iteration 205: weight=1.0000349601293788:loss=1.339822919277367e-09\n",
            "iteration 206: weight=1.0000333708180724:loss=1.2222105549852813e-09\n",
            "iteration 207: weight=1.000031835189975:loss=1.1136114252963125e-09\n",
            "iteration 208: weight=1.000030352661311:loss=1.013479300482345e-09\n",
            "iteration 209: weight=1.0000289225447119:loss=9.212839380268179e-10\n",
            "iteration 210: weight=1.000027544061439:loss=8.365135251153788e-10\n",
            "iteration 211: weight=1.0000262163526807:loss=7.586753447696992e-10\n",
            "iteration 212: weight=1.0000249384899804:loss=6.872971081151036e-10\n",
            "iteration 213: weight=1.0000237094848503:loss=6.21928286648199e-10\n",
            "iteration 214: weight=1.0000225282976234:loss=5.621396681476654e-10\n",
            "iteration 215: weight=1.00002139384559:loss=5.075242448526751e-10\n",
            "iteration 216: weight=1.000020305010467:loss=4.576965473290784e-10\n",
            "iteration 217: weight=1.0000192606452416:loss=4.1229353264782276e-10\n",
            "iteration 218: weight=1.000018259580431:loss=3.70972363938904e-10\n",
            "iteration 219: weight=1.0000173006297972:loss=3.334121867482054e-10\n",
            "iteration 220: weight=1.0000163825955528:loss=2.993116865468437e-10\n",
            "iteration 221: weight=1.000015504273092:loss=2.683895328203789e-10\n",
            "iteration 222: weight=1.0000146644552799:loss=2.4038238066736994e-10\n",
            "iteration 223: weight=1.000013861936329:loss=2.1504620306700417e-10\n",
            "iteration 224: weight=1.0000130955152955:loss=1.9215318225462852e-10\n",
            "iteration 225: weight=1.0000123639992167:loss=1.7149259790016913e-10\n",
            "iteration 226: weight=1.0000116662059206:loss=1.528683846174772e-10\n",
            "iteration 227: weight=1.0000110009665282:loss=1.3610046423195854e-10\n",
            "iteration 228: weight=1.0000103671276706:loss=1.2102119306689474e-10\n",
            "iteration 229: weight=1.0000097635534464:loss=1.0747736034488753e-10\n",
            "iteration 230: weight=1.0000091891271334:loss=9.532707956338982e-11\n",
            "iteration 231: weight=1.0000086427526778:loss=8.44400105393106e-11\n",
            "iteration 232: weight=1.000008123355975:loss=7.469713736441008e-11\n",
            "iteration 233: weight=1.0000076298859621:loss=6.59889920484602e-11\n",
            "iteration 234: weight=1.0000071613155337:loss=5.821521043003486e-11\n",
            "iteration 235: weight=1.0000067166422966:loss=5.1284532176509856e-11\n",
            "iteration 236: weight=1.0000062948891788:loss=4.511324647182846e-11\n",
            "iteration 237: weight=1.000005895104903:loss=3.962563610571124e-11\n",
            "iteration 238: weight=1.0000055163643353:loss=3.475220111681665e-11\n",
            "iteration 239: weight=1.0000051577687257:loss=3.043032492655584e-11\n",
            "iteration 240: weight=1.0000048184458419:loss=2.6602497982253226e-11\n",
            "iteration 241: weight=1.0000044975500146:loss=2.3217427980171124e-11\n",
            "iteration 242: weight=1.0000041942620974:loss=2.0228041464065427e-11\n",
            "iteration 243: weight=1.0000039077893508:loss=1.7591927914395455e-11\n",
            "iteration 244: weight=1.0000036373652605:loss=1.5270895659114103e-11\n",
            "iteration 245: weight=1.0000033822492937:loss=1.323052778445799e-11\n",
            "iteration 246: weight=1.0000031417266018:loss=1.1439516001132688e-11\n",
            "iteration 247: weight=1.0000029151076746:loss=9.870548822732417e-12\n",
            "iteration 248: weight=1.0000027017279542:loss=8.497869075085873e-12\n",
            "iteration 249: weight=1.00000250094741:loss=7.299272297700554e-12\n",
            "iteration 250: weight=1.000002312150083:loss=6.254774476133207e-12\n",
            "iteration 251: weight=1.000002134743601:loss=5.345945908175054e-12\n",
            "iteration 252: weight=1.000001968158672:loss=4.5570214268764175e-12\n",
            "iteration 253: weight=1.000001811848557:loss=3.873568132917171e-12\n",
            "iteration 254: weight=1.0000016652885257:loss=3.282707439211663e-12\n",
            "iteration 255: weight=1.0000015279753016:loss=2.773115070908716e-12\n",
            "iteration 256: weight=1.000001399426494:loss=2.3347990207867042e-12\n",
            "iteration 257: weight=1.0000012791800243:loss=1.958433415438776e-12\n",
            "iteration 258: weight=1.0000011667935476:loss=1.6362466936925557e-12\n",
            "iteration 259: weight=1.0000010618438693:loss=1.361355472795367e-12\n",
            "iteration 260: weight=1.0000009639263634:loss=1.127542503809309e-12\n",
            "iteration 261: weight=1.0000008726543899:loss=9.29256671611256e-13\n",
            "iteration 262: weight=1.000000787658716:loss=7.616129948928574e-13\n",
            "iteration 263: weight=1.0000007085869405:loss=6.203926261605375e-13\n",
            "iteration 264: weight=1.0000006351029236:loss=5.020428517354958e-13\n",
            "iteration 265: weight=1.0000005668862242:loss=4.034550471487819e-13\n",
            "iteration 266: weight=1.0000005036315436:loss=3.212985433265203e-13\n",
            "iteration 267: weight=1.0000004450481785:loss=2.5357493882438575e-13\n",
            "iteration 268: weight=1.0000003908594828:loss=1.9806378759312793e-13\n",
            "iteration 269: weight=1.0000003408023395:loss=1.5276668818842154e-13\n",
            "iteration 270: weight=1.0000002946266435:loss=1.1612932837579137e-13\n",
            "iteration 271: weight=1.0000002520947957:loss=8.681944052568724e-14\n",
            "iteration 272: weight=1.0000002129812071:loss=6.350475700855895e-14\n",
            "iteration 273: weight=1.0000001770718168:loss=4.529709940470639e-14\n",
            "iteration 274: weight=1.000000144163621:loss=3.1308289294429414e-14\n",
            "iteration 275: weight=1.0000001140642147:loss=2.0872192862952943e-14\n",
            "iteration 276: weight=1.0000000865913463:loss=1.3100631690576847e-14\n",
            "iteration 277: weight=1.0000000615724836:loss=7.549516567451064e-15\n",
            "iteration 278: weight=1.0000000388443944:loss=3.774758283725532e-15\n",
            "iteration 279: weight=1.0000000182527384:loss=1.5543122344752192e-15\n",
            "iteration 280: weight=0.9999999996516727:loss=4.440892098500626e-16\n",
            "iteration 281: weight=0.999999982903469:loss=0.0\n",
            "iteration 282: weight=0.9999999678781447:loss=3.3306690738754696e-16\n",
            "iteration 283: weight=0.9999999544531054:loss=9.992007221626409e-16\n",
            "iteration 284: weight=0.9999999425128001:loss=2.1094237467877974e-15\n",
            "iteration 285: weight=0.999999931948389:loss=3.3306690738754696e-15\n",
            "iteration 286: weight=0.9999999226574222:loss=4.6629367034256575e-15\n",
            "iteration 287: weight=0.9999999145435317:loss=5.995204332975845e-15\n",
            "iteration 288: weight=0.9999999075161332:loss=7.327471962526033e-15\n",
            "iteration 289: weight=0.9999999014901411:loss=8.548717289613705e-15\n",
            "iteration 290: weight=0.9999998963856931:loss=9.658940314238862e-15\n",
            "iteration 291: weight=0.9999998921278863:loss=1.0769163338864018e-14\n",
            "iteration 292: weight=0.9999998886465239:loss=1.1657341758564144e-14\n",
            "iteration 293: weight=0.9999998858758722:loss=1.2434497875801753e-14\n",
            "iteration 294: weight=0.9999998837544273:loss=1.2989609388114332e-14\n",
            "iteration 295: weight=0.999999882224692:loss=1.354472090042691e-14\n",
            "iteration 296: weight=0.9999998812329625:loss=1.3877787807814457e-14\n",
            "iteration 297: weight=0.9999998807291227:loss=1.4099832412739488e-14\n",
            "iteration 298: weight=0.9999998806664501:loss=1.4210854715202004e-14\n",
            "iteration 299: weight=0.9999998810014278:loss=1.4210854715202004e-14\n",
            "iteration 300: weight=0.9999998816935664:loss=1.4210854715202004e-14\n",
            "iteration 301: weight=0.9999998827052335:loss=1.3988810110276972e-14\n",
            "iteration 302: weight=0.9999998840014915:loss=1.3766765505351941e-14\n",
            "iteration 303: weight=0.999999885549942:loss=1.3433698597964394e-14\n",
            "iteration 304: weight=0.9999998873205784:loss=1.3100631690576847e-14\n",
            "iteration 305: weight=0.9999998892856454:loss=1.2656542480726785e-14\n",
            "iteration 306: weight=0.999999891419505:loss=1.2212453270876722e-14\n",
            "iteration 307: weight=0.9999998936985092:loss=1.176836406102666e-14\n",
            "iteration 308: weight=0.999999896100879:loss=1.1324274851176597e-14\n",
            "iteration 309: weight=0.9999998986065896:loss=1.0769163338864018e-14\n",
            "iteration 310: weight=0.9999999011972606:loss=1.0325074129013956e-14\n",
            "iteration 311: weight=0.9999999038560531:loss=9.769962616701378e-15\n",
            "iteration 312: weight=0.9999999065675711:loss=9.2148511043888e-15\n",
            "iteration 313: weight=0.9999999093177684:loss=8.770761894538737e-15\n",
            "iteration 314: weight=0.9999999120938603:loss=8.215650382226158e-15\n",
            "iteration 315: weight=0.9999999148842403:loss=7.771561172376096e-15\n",
            "iteration 316: weight=0.999999917678401:loss=7.216449660063518e-15\n",
            "iteration 317: weight=0.9999999204668591:loss=6.772360450213455e-15\n",
            "iteration 318: weight=0.9999999232410854:loss=6.328271240363392e-15\n",
            "iteration 319: weight=0.9999999259934375:loss=5.88418203051333e-15\n",
            "iteration 320: weight=0.999999928717097:loss=5.440092820663267e-15\n",
            "iteration 321: weight=0.9999999314060107:loss=5.10702591327572e-15\n",
            "iteration 322: weight=0.9999999340548343:loss=4.6629367034256575e-15\n",
            "iteration 323: weight=0.9999999366588797:loss=4.3298697960381105e-15\n",
            "iteration 324: weight=0.999999939214065:loss=3.9968028886505635e-15\n",
            "iteration 325: weight=0.9999999417168692:loss=3.6637359812630166e-15\n",
            "iteration 326: weight=0.999999944164287:loss=3.4416913763379853e-15\n",
            "iteration 327: weight=0.9999999465537882:loss=3.1086244689504383e-15\n",
            "iteration 328: weight=0.9999999488832795:loss=2.886579864025407e-15\n",
            "iteration 329: weight=0.9999999511510674:loss=2.6645352591003757e-15\n",
            "iteration 330: weight=0.9999999533558253:loss=2.3314683517128287e-15\n",
            "iteration 331: weight=0.9999999554965608:loss=2.220446049250313e-15\n",
            "iteration 332: weight=0.9999999575725869:loss=1.9984014443252818e-15\n",
            "iteration 333: weight=0.9999999595834933:loss=1.7763568394002505e-15\n",
            "iteration 334: weight=0.9999999615291214:loss=1.6653345369377348e-15\n",
            "iteration 335: weight=0.9999999634095394:loss=1.4432899320127035e-15\n",
            "iteration 336: weight=0.9999999652250204:loss=1.3322676295501878e-15\n",
            "iteration 337: weight=0.999999966976021:loss=1.2212453270876722e-15\n",
            "iteration 338: weight=0.999999968663162:loss=1.1102230246251565e-15\n",
            "iteration 339: weight=0.9999999702872102:loss=9.992007221626409e-16\n",
            "iteration 340: weight=0.9999999718490618:loss=8.881784197001252e-16\n",
            "iteration 341: weight=0.9999999733497265:loss=7.771561172376096e-16\n",
            "iteration 342: weight=0.9999999747903134:loss=6.661338147750939e-16\n",
            "iteration 343: weight=0.9999999761720179:loss=6.661338147750939e-16\n",
            "iteration 344: weight=0.9999999774961086:loss=5.551115123125783e-16\n",
            "iteration 345: weight=0.999999978763917:loss=5.551115123125783e-16\n",
            "iteration 346: weight=0.9999999799768264:loss=4.440892098500626e-16\n",
            "iteration 347: weight=0.9999999811362628:loss=4.440892098500626e-16\n",
            "iteration 348: weight=0.9999999822436856:loss=3.3306690738754696e-16\n",
            "iteration 349: weight=0.9999999833005803:loss=3.3306690738754696e-16\n",
            "iteration 350: weight=0.9999999843084506:loss=3.3306690738754696e-16\n",
            "iteration 351: weight=0.9999999852688121:loss=2.220446049250313e-16\n",
            "iteration 352: weight=0.999999986183186:loss=2.220446049250313e-16\n",
            "iteration 353: weight=0.999999987053094:loss=2.220446049250313e-16\n",
            "iteration 354: weight=0.9999999878800532:loss=2.220446049250313e-16\n",
            "iteration 355: weight=0.9999999886655714:loss=1.1102230246251565e-16\n",
            "iteration 356: weight=0.9999999894111435:loss=1.1102230246251565e-16\n",
            "iteration 357: weight=0.9999999901182479:loss=1.1102230246251565e-16\n",
            "iteration 358: weight=0.999999990788343:loss=1.1102230246251565e-16\n",
            "iteration 359: weight=0.999999991422865:loss=1.1102230246251565e-16\n",
            "iteration 360: weight=0.9999999920232251:loss=1.1102230246251565e-16\n",
            "iteration 361: weight=0.9999999925908074:loss=1.1102230246251565e-16\n",
            "iteration 362: weight=0.9999999931269672:loss=0.0\n",
            "iteration 363: weight=0.9999999936330293:loss=0.0\n",
            "iteration 364: weight=0.9999999941102867:loss=0.0\n",
            "iteration 365: weight=0.99999999456:loss=0.0\n",
            "iteration 366: weight=0.9999999949833956:loss=0.0\n",
            "iteration 367: weight=0.9999999953816662:loss=0.0\n",
            "iteration 368: weight=0.999999995755969:loss=0.0\n",
            "iteration 369: weight=0.9999999961074265:loss=0.0\n",
            "iteration 370: weight=0.9999999964371257:loss=0.0\n",
            "iteration 371: weight=0.9999999967461182:loss=0.0\n",
            "iteration 372: weight=0.9999999970354202:loss=0.0\n",
            "iteration 373: weight=0.9999999973060127:loss=0.0\n",
            "iteration 374: weight=0.999999997558842:loss=0.0\n",
            "iteration 375: weight=0.9999999977948193:loss=0.0\n",
            "iteration 376: weight=0.9999999980148222:loss=0.0\n",
            "iteration 377: weight=0.9999999982196943:loss=0.0\n",
            "iteration 378: weight=0.9999999984102462:loss=0.0\n",
            "iteration 379: weight=0.999999998587256:loss=0.0\n",
            "iteration 380: weight=0.9999999987514698:loss=0.0\n",
            "iteration 381: weight=0.9999999989036029:loss=0.0\n",
            "iteration 382: weight=0.9999999990443398:loss=0.0\n",
            "iteration 383: weight=0.9999999991743355:loss=0.0\n",
            "iteration 384: weight=0.9999999992942159:loss=0.0\n",
            "iteration 385: weight=0.999999999404579:loss=0.0\n",
            "iteration 386: weight=0.9999999995059953:loss=0.0\n",
            "iteration 387: weight=0.9999999995990089:loss=0.0\n",
            "iteration 388: weight=0.9999999996841381:loss=0.0\n",
            "iteration 389: weight=0.9999999997618765:loss=0.0\n",
            "iteration 390: weight=0.9999999998326935:loss=0.0\n",
            "iteration 391: weight=0.9999999998970353:loss=0.0\n",
            "iteration 392: weight=0.9999999999553256:loss=0.0\n",
            "iteration 393: weight=1.0000000000079667:loss=0.0\n",
            "iteration 394: weight=1.0000000000553402:loss=0.0\n",
            "iteration 395: weight=1.000000000097807:loss=0.0\n",
            "iteration 396: weight=1.0000000001357097:loss=0.0\n",
            "iteration 397: weight=1.0000000001693716:loss=0.0\n",
            "iteration 398: weight=1.0000000001990987:loss=0.0\n",
            "iteration 399: weight=1.00000000022518:loss=0.0\n",
            "iteration 400: weight=1.0000000002478882:loss=0.0\n",
            "iteration 401: weight=1.00000000026748:loss=0.0\n",
            "iteration 402: weight=1.0000000002841978:loss=0.0\n",
            "iteration 403: weight=1.0000000002982694:loss=0.0\n",
            "iteration 404: weight=1.000000000309909:loss=0.0\n",
            "iteration 405: weight=1.0000000003193181:loss=0.0\n",
            "iteration 406: weight=1.0000000003266856:loss=0.0\n",
            "iteration 407: weight=1.0000000003321885:loss=0.0\n",
            "iteration 408: weight=1.000000000335993:loss=0.0\n",
            "iteration 409: weight=1.0000000003382545:loss=0.0\n",
            "iteration 410: weight=1.000000000339118:loss=0.0\n",
            "iteration 411: weight=1.0000000003387193:loss=0.0\n",
            "iteration 412: weight=1.0000000003371845:loss=0.0\n",
            "iteration 413: weight=1.0000000003346317:loss=0.0\n",
            "iteration 414: weight=1.0000000003311704:loss=0.0\n",
            "iteration 415: weight=1.0000000003269025:loss=0.0\n",
            "iteration 416: weight=1.0000000003219227:loss=0.0\n",
            "iteration 417: weight=1.0000000003163185:loss=0.0\n",
            "iteration 418: weight=1.000000000310171:loss=0.0\n",
            "iteration 419: weight=1.0000000003035552:loss=0.0\n",
            "iteration 420: weight=1.0000000002965401:loss=0.0\n",
            "iteration 421: weight=1.0000000002891896:loss=0.0\n",
            "iteration 422: weight=1.000000000281562:loss=0.0\n",
            "iteration 423: weight=1.0000000002737113:loss=0.0\n",
            "iteration 424: weight=1.0000000002656864:loss=0.0\n",
            "iteration 425: weight=1.0000000002575322:loss=0.0\n",
            "iteration 426: weight=1.0000000002492897:loss=0.0\n",
            "iteration 427: weight=1.000000000240996:loss=0.0\n",
            "iteration 428: weight=1.0000000002326845:loss=0.0\n",
            "iteration 429: weight=1.0000000002243863:loss=0.0\n",
            "iteration 430: weight=1.0000000002161282:loss=0.0\n",
            "iteration 431: weight=1.000000000207935:loss=0.0\n",
            "iteration 432: weight=1.0000000001998284:loss=0.0\n",
            "iteration 433: weight=1.0000000001918279:loss=0.0\n",
            "iteration 434: weight=1.0000000001839506:loss=0.0\n",
            "iteration 435: weight=1.0000000001762117:loss=0.0\n",
            "iteration 436: weight=1.0000000001686242:loss=0.0\n",
            "iteration 437: weight=1.0000000001611995:loss=0.0\n",
            "iteration 438: weight=1.000000000153947:loss=0.0\n",
            "iteration 439: weight=1.0000000001468752:loss=0.0\n",
            "iteration 440: weight=1.0000000001399902:loss=0.0\n",
            "iteration 441: weight=1.0000000001332978:loss=0.0\n",
            "iteration 442: weight=1.000000000126802:loss=0.0\n",
            "iteration 443: weight=1.0000000001205056:loss=0.0\n",
            "iteration 444: weight=1.0000000001144111:loss=0.0\n",
            "iteration 445: weight=1.0000000001085196:loss=0.0\n",
            "iteration 446: weight=1.0000000001028315:loss=0.0\n",
            "iteration 447: weight=1.0000000000973466:loss=0.0\n",
            "iteration 448: weight=1.0000000000920637:loss=0.0\n",
            "iteration 449: weight=1.000000000086981:loss=0.0\n",
            "iteration 450: weight=1.0000000000820968:loss=0.0\n",
            "iteration 451: weight=1.0000000000774083:loss=0.0\n",
            "iteration 452: weight=1.0000000000729123:loss=0.0\n",
            "iteration 453: weight=1.0000000000686058:loss=0.0\n",
            "iteration 454: weight=1.0000000000644846:loss=0.0\n",
            "iteration 455: weight=1.0000000000605451:loss=0.0\n",
            "iteration 456: weight=1.000000000056783:loss=0.0\n",
            "iteration 457: weight=1.000000000053194:loss=0.0\n",
            "iteration 458: weight=1.000000000049773:loss=0.0\n",
            "iteration 459: weight=1.000000000046516:loss=0.0\n",
            "iteration 460: weight=1.0000000000434177:loss=0.0\n",
            "iteration 461: weight=1.0000000000404734:loss=0.0\n",
            "iteration 462: weight=1.0000000000376783:loss=0.0\n",
            "iteration 463: weight=1.0000000000350273:loss=0.0\n",
            "iteration 464: weight=1.0000000000325153:loss=0.0\n",
            "iteration 465: weight=1.0000000000301377:loss=0.0\n",
            "iteration 466: weight=1.0000000000278892:loss=0.0\n",
            "iteration 467: weight=1.0000000000257652:loss=0.0\n",
            "iteration 468: weight=1.0000000000237605:loss=0.0\n",
            "iteration 469: weight=1.0000000000218705:loss=0.0\n",
            "iteration 470: weight=1.0000000000200906:loss=0.0\n",
            "iteration 471: weight=1.000000000018416:loss=0.0\n",
            "iteration 472: weight=1.000000000016842:loss=0.0\n",
            "iteration 473: weight=1.0000000000153648:loss=0.0\n",
            "iteration 474: weight=1.0000000000139795:loss=0.0\n",
            "iteration 475: weight=1.000000000012682:loss=0.0\n",
            "iteration 476: weight=1.0000000000114684:loss=0.0\n",
            "iteration 477: weight=1.0000000000103344:loss=0.0\n",
            "iteration 478: weight=1.0000000000092761:loss=0.0\n",
            "iteration 479: weight=1.00000000000829:loss=0.0\n",
            "iteration 480: weight=1.0000000000073723:loss=0.0\n",
            "iteration 481: weight=1.0000000000065195:loss=0.0\n",
            "iteration 482: weight=1.000000000005728:loss=0.0\n",
            "iteration 483: weight=1.000000000004995:loss=0.0\n",
            "iteration 484: weight=1.0000000000043168:loss=0.0\n",
            "iteration 485: weight=1.0000000000036906:loss=0.0\n",
            "iteration 486: weight=1.0000000000031135:loss=0.0\n",
            "iteration 487: weight=1.0000000000025826:loss=0.0\n",
            "iteration 488: weight=1.0000000000020952:loss=0.0\n",
            "iteration 489: weight=1.0000000000016487:loss=0.0\n",
            "iteration 490: weight=1.0000000000012408:loss=0.0\n",
            "iteration 491: weight=1.000000000000869:loss=0.0\n",
            "iteration 492: weight=1.0000000000005311:loss=0.0\n",
            "iteration 493: weight=1.000000000000225:loss=0.0\n",
            "iteration 494: weight=0.9999999999999484:loss=0.0\n",
            "iteration 495: weight=0.9999999999996996:loss=0.0\n",
            "iteration 496: weight=0.9999999999994766:loss=0.0\n",
            "iteration 497: weight=0.9999999999992778:loss=0.0\n",
            "iteration 498: weight=0.9999999999991014:loss=0.0\n",
            "iteration 499: weight=0.9999999999989457:loss=0.0\n",
            "iteration 500: weight=0.9999999999988094:loss=0.0\n",
            "iteration 501: weight=0.9999999999986909:loss=0.0\n",
            "iteration 502: weight=0.999999999998589:loss=0.0\n",
            "iteration 503: weight=0.9999999999985024:loss=0.0\n",
            "iteration 504: weight=0.9999999999984298:loss=0.0\n",
            "iteration 505: weight=0.9999999999983702:loss=0.0\n",
            "iteration 506: weight=0.9999999999983225:loss=0.0\n",
            "iteration 507: weight=0.9999999999982856:loss=0.0\n",
            "iteration 508: weight=0.9999999999982586:loss=0.0\n",
            "iteration 509: weight=0.9999999999982406:loss=0.0\n",
            "iteration 510: weight=0.9999999999982309:loss=0.0\n",
            "iteration 511: weight=0.9999999999982285:loss=0.0\n",
            "iteration 512: weight=0.9999999999982329:loss=0.0\n",
            "iteration 513: weight=0.9999999999982433:loss=0.0\n",
            "iteration 514: weight=0.9999999999982591:loss=0.0\n",
            "iteration 515: weight=0.9999999999982796:loss=0.0\n",
            "iteration 516: weight=0.9999999999983044:loss=0.0\n",
            "iteration 517: weight=0.9999999999983329:loss=0.0\n",
            "iteration 518: weight=0.9999999999983646:loss=0.0\n",
            "iteration 519: weight=0.9999999999983993:loss=0.0\n",
            "iteration 520: weight=0.9999999999984364:loss=0.0\n",
            "iteration 521: weight=0.9999999999984754:loss=0.0\n",
            "iteration 522: weight=0.9999999999985162:loss=0.0\n",
            "iteration 523: weight=0.9999999999985584:loss=0.0\n",
            "iteration 524: weight=0.9999999999986017:loss=0.0\n",
            "iteration 525: weight=0.9999999999986457:loss=0.0\n",
            "iteration 526: weight=0.9999999999986904:loss=0.0\n",
            "iteration 527: weight=0.9999999999987355:loss=0.0\n",
            "iteration 528: weight=0.9999999999987806:loss=0.0\n",
            "iteration 529: weight=0.9999999999988258:loss=0.0\n",
            "iteration 530: weight=0.9999999999988709:loss=0.0\n",
            "iteration 531: weight=0.9999999999989156:loss=0.0\n",
            "iteration 532: weight=0.9999999999989599:loss=0.0\n",
            "iteration 533: weight=0.9999999999990036:loss=0.0\n",
            "iteration 534: weight=0.9999999999990465:loss=0.0\n",
            "iteration 535: weight=0.9999999999990887:loss=0.0\n",
            "iteration 536: weight=0.9999999999991301:loss=0.0\n",
            "iteration 537: weight=0.9999999999991707:loss=0.0\n",
            "iteration 538: weight=0.9999999999992102:loss=0.0\n",
            "iteration 539: weight=0.9999999999992487:loss=0.0\n",
            "iteration 540: weight=0.9999999999992861:loss=0.0\n",
            "iteration 541: weight=0.9999999999993225:loss=0.0\n",
            "iteration 542: weight=0.9999999999993578:loss=0.0\n",
            "iteration 543: weight=0.9999999999993919:loss=0.0\n",
            "iteration 544: weight=0.9999999999994249:loss=0.0\n",
            "iteration 545: weight=0.9999999999994568:loss=0.0\n",
            "iteration 546: weight=0.9999999999994874:loss=0.0\n",
            "iteration 547: weight=0.9999999999995169:loss=0.0\n",
            "iteration 548: weight=0.9999999999995454:loss=0.0\n",
            "iteration 549: weight=0.9999999999995726:loss=0.0\n",
            "iteration 550: weight=0.9999999999995987:loss=0.0\n",
            "iteration 551: weight=0.9999999999996236:loss=0.0\n",
            "iteration 552: weight=0.9999999999996476:loss=0.0\n",
            "iteration 553: weight=0.9999999999996705:loss=0.0\n",
            "iteration 554: weight=0.9999999999996922:loss=0.0\n",
            "iteration 555: weight=0.999999999999713:loss=0.0\n",
            "iteration 556: weight=0.9999999999997328:loss=0.0\n",
            "iteration 557: weight=0.9999999999997515:loss=0.0\n",
            "iteration 558: weight=0.9999999999997694:loss=0.0\n",
            "iteration 559: weight=0.9999999999997864:loss=0.0\n",
            "iteration 560: weight=0.9999999999998025:loss=0.0\n",
            "iteration 561: weight=0.9999999999998177:loss=0.0\n",
            "iteration 562: weight=0.999999999999832:loss=0.0\n",
            "iteration 563: weight=0.9999999999998456:loss=0.0\n",
            "iteration 564: weight=0.9999999999998583:loss=0.0\n",
            "iteration 565: weight=0.9999999999998704:loss=0.0\n",
            "iteration 566: weight=0.9999999999998818:loss=0.0\n",
            "iteration 567: weight=0.9999999999998924:loss=0.0\n",
            "iteration 568: weight=0.9999999999999024:loss=0.0\n",
            "iteration 569: weight=0.9999999999999117:loss=0.0\n",
            "iteration 570: weight=0.9999999999999205:loss=0.0\n",
            "iteration 571: weight=0.9999999999999287:loss=0.0\n",
            "iteration 572: weight=0.9999999999999364:loss=0.0\n",
            "iteration 573: weight=0.9999999999999435:loss=0.0\n",
            "iteration 574: weight=0.9999999999999502:loss=0.0\n",
            "iteration 575: weight=0.9999999999999563:loss=0.0\n",
            "iteration 576: weight=0.9999999999999619:loss=0.0\n",
            "iteration 577: weight=0.9999999999999672:loss=0.0\n",
            "iteration 578: weight=0.9999999999999721:loss=0.0\n",
            "iteration 579: weight=0.9999999999999767:loss=0.0\n",
            "iteration 580: weight=0.9999999999999808:loss=0.0\n",
            "iteration 581: weight=0.9999999999999846:loss=0.0\n",
            "iteration 582: weight=0.999999999999988:loss=0.0\n",
            "iteration 583: weight=0.9999999999999912:loss=0.0\n",
            "iteration 584: weight=0.9999999999999941:loss=0.0\n",
            "iteration 585: weight=0.9999999999999968:loss=0.0\n",
            "iteration 586: weight=0.9999999999999991:loss=0.0\n",
            "iteration 587: weight=1.0000000000000013:loss=0.0\n",
            "iteration 588: weight=1.0000000000000033:loss=0.0\n",
            "iteration 589: weight=1.000000000000005:loss=0.0\n",
            "iteration 590: weight=1.0000000000000067:loss=0.0\n",
            "iteration 591: weight=1.000000000000008:loss=0.0\n",
            "iteration 592: weight=1.000000000000009:loss=0.0\n",
            "iteration 593: weight=1.0000000000000102:loss=0.0\n",
            "iteration 594: weight=1.000000000000011:loss=0.0\n",
            "iteration 595: weight=1.0000000000000118:loss=0.0\n",
            "iteration 596: weight=1.0000000000000124:loss=0.0\n",
            "iteration 597: weight=1.0000000000000129:loss=0.0\n",
            "iteration 598: weight=1.0000000000000133:loss=0.0\n",
            "iteration 599: weight=1.0000000000000138:loss=0.0\n",
            "iteration 600: weight=1.000000000000014:loss=0.0\n",
            "iteration 601: weight=1.0000000000000142:loss=0.0\n",
            "converged after 602 iterations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in sqrt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(x,l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "5Y2CUqoFnNhk",
        "outputId": "7f620b2a-d688-4a0d-ed0c-141628550301"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f93d11f6750>]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYgUlEQVR4nO3df5BV533f8fdnf/9il132SiBYCZBJ5ZUjS8oKy6PUdWzLRakDbmOnMO3UnqplMjWNW2eaSk1LXfUvOxO76ZQ6JqkaT1oHy2oSb20a4lpy2jSxzCqSsQAjrRAyiySzwPKbhf3x7R/3LLpaLewF7u7dc87nNbPDPec83Pt9pOWzzz7nnOcoIjAzs/SrqXYBZmZWGQ50M7OMcKCbmWWEA93MLCMc6GZmGVFXrQ/u7u6OlStXVuvjzcxS6dlnnz0WEYWZjlUt0FeuXMnAwEC1Pt7MLJUkvXqlY55yMTPLCAe6mVlGONDNzDLCgW5mlhEOdDOzjHCgm5llhAPdzCwjUhfouw+d4HN/8iO87K+Z2VulLtD3DJ3iS999mZPnx6pdipnZglJWoEtaJ+mApEFJj8xw/IuSnk++XpR0svKlFi3raALg9VOjc/URZmapNOut/5JqgW3Ag8AQsFtSf0Tsm2oTEf+8pP0/Be6Zg1oBWJoE+hunL9B7S/tcfYyZWeqUM0JfCwxGxMGIuATsADZcpf0m4A8qUdxMpkbob5y6OFcfYWaWSuUE+nLgcMn2ULLvbSTdBqwCnrrC8c2SBiQNDA8PX2utABTaGqkRvH7qwnX9fTOzrKr0SdGNwJMRMTHTwYjYHhF9EdFXKMy4+uOs6mpr6Gpt5NjZSzdSp5lZ5pQT6EeAnpLtFcm+mWxkDqdbpnS21HPyvAPdzKxUOYG+G1gjaZWkBoqh3T+9kaQ7gE7gLytb4tt1tjZw4pwD3cys1KyBHhHjwBZgF7AfeCIi9kp6TNL6kqYbgR0xD3f8dLU0MOIRupnZW5T1xKKI2AnsnLZv67Ttz1aurKvrbK1n5Me+scjMrFTq7hQF6GxpYOTcJd/+b2ZWIrWBPj4ZnLk4Xu1SzMwWjHQGemsDACfPedrFzGxKKgO9q7UegBM+MWpmdlkqA31xS3GEPuJLF83MLktloHdNBbpH6GZml6Uy0Kfm0H1zkZnZm1IZ6O1NddTWyCN0M7MSqQx0SSxurvdTi8zMSqQy0AHam+s5Perr0M3MpqQ60E9d8AjdzGxKegO9qY7TDnQzs8vSG+jN9Q50M7MSqQ30juZ6To860M3MpqQ20NubinPoXnHRzKwotYHe0VzP2EQwOjZZ7VLMzBaE1AZ6e3Px2RyedjEzK0pvoDcVV1z0pYtmZkVlBbqkdZIOSBqU9MgV2vySpH2S9kr6amXLfLuO5mKg+0oXM7OiWZ8pKqkW2AY8CAwBuyX1R8S+kjZrgEeBByJiRNJNc1XwlPZmj9DNzEqVM0JfCwxGxMGIuATsADZMa/OPgW0RMQIQEUcrW+bbXR6hew7dzAwoL9CXA4dLtoeSfaV+CvgpSf9P0vckrZvpjSRtljQgaWB4ePj6Kk60NxV/uTjlBbrMzIDKnRStA9YA7wc2Ab8jafH0RhGxPSL6IqKvUCjc0Ae2Xx6he4EuMzMoL9CPAD0l2yuSfaWGgP6IGIuIV4AXKQb8nKmvraGlodZz6GZmiXICfTewRtIqSQ3ARqB/Wps/pjg6R1I3xSmYgxWsc0btTV7PxcxsyqyBHhHjwBZgF7AfeCIi9kp6TNL6pNku4LikfcDTwL+IiONzVfQUr+diZvamWS9bBIiIncDOafu2lrwO4DPJ17xpb67zlIuZWSK1d4pCMkK/4JOiZmaQ8kCfWnHRzMzSHuieQzczuyz1gX5mdJyJSa+JbmaW7kBP7hY965uLzMzSHegdXqDLzOyyVAd6uxfoMjO7LNWB7jXRzczelOpA91OLzMzelOpA72jxlIuZ2ZRUB/rlNdE9QjczS3egtzbUUSN8+7+ZGSkP9Joa0d7s2//NzCDlgQ7JmuieQzczS3+gd3iEbmYGZCDQ25vrfB26mRlZCPSmej8o2syMDAS6p1zMzIpSH+jtzX5QtJkZlBnoktZJOiBpUNIjMxz/pKRhSc8nX/+o8qXOrKO5novjk4yOTczXR5qZLUizPiRaUi2wDXgQGAJ2S+qPiH3Tmn4tIrbMQY1XNXW36OkLYzTV1873x5uZLRjljNDXAoMRcTAiLgE7gA1zW1b52r0mupkZUF6gLwcOl2wPJfum+0VJeyQ9KalnpjeStFnSgKSB4eHh6yj37Ra3NAAOdDOzSp0U/Z/Ayoi4C/g28JWZGkXE9ojoi4i+QqFQkQ/2U4vMzIrKCfQjQOmIe0Wy77KIOB4RF5PN3wV+pjLlzc6BbmZWVE6g7wbWSFolqQHYCPSXNpC0rGRzPbC/ciVe3VSgnzzvQDezfJv1KpeIGJe0BdgF1AKPR8ReSY8BAxHRD/yKpPXAOHAC+OQc1vwWXhPdzKxo1kAHiIidwM5p+7aWvH4UeLSypZWnrraGRY11DnQzy73U3ykKeE10MzMyEuiLWxzoZmaZCHQv0GVmlqFAP3n+UrXLMDOrqkwEenHKxWuim1m+ZSLQp5bQjYhql2JmVjWZCPSO5nouTUwyOjZZ7VLMzKomE4G+uLm4QNfJC55HN7P8ykSgez0XM7OsBbrXczGzHMtEoC9uSRbo8gjdzHIsE4HuKRczs4wE+tRj6E470M0sxzIR6Isa66iR10Q3s3zLRKDX1MgrLppZ7mUi0MELdJmZZSbQFzvQzSznMhPo7c31vmzRzHItM4HekSzQZWaWV2UFuqR1kg5IGpT0yFXa/aKkkNRXuRLL46cWmVnezRrokmqBbcBDQC+wSVLvDO0WAZ8Gnql0keWYOinqJXTNLK/KGaGvBQYj4mBEXAJ2ABtmaPfvgc8BoxWsr2wdzfVMTAZnL/pBF2aWT+UE+nLgcMn2ULLvMkn3Aj0R8a2rvZGkzZIGJA0MDw9fc7FXc3kJXd9cZGY5dcMnRSXVAF8AfnW2thGxPSL6IqKvUCjc6Ee/xeUFuhzoZpZT5QT6EaCnZHtFsm/KIuBdwHclHQLuB/rn+8RoV2txhD7ih0WbWU6VE+i7gTWSVklqADYC/VMHI+JURHRHxMqIWAl8D1gfEQNzUvEVdDrQzSznZg30iBgHtgC7gP3AExGxV9JjktbPdYHl6mopBvqJcw50M8ununIaRcROYOe0fVuv0Pb9N17WtWtvrkeCEQe6meVUZu4Ura0Ri5vrGfFJUTPLqcwEOhTn0U94Dt3McipTgd7V0uApFzPLrUwF+uKWBp8UNbPcylSgd7XW+8YiM8utTAX61By6F+gyszzKVKB3tTRwaXyS85cmql2Kmdm8y1SgT90t6nl0M8ujbAV6i1dcNLP8ylSgd7UWV1z0tehmlkeZCvSpEbqvRTezPMpUoHd5Dt3McixTgd7eVE+N4KSnXMwshzIV6DU1Kt4t6kA3sxzKVKADdLbUe8rFzHIpc4He3dbIsTMOdDPLn+wF+qJGjp29WO0yzMzmXeYCvdDWyLAD3cxyKHOBvqS1gTOj44yOeT0XM8uXsgJd0jpJByQNSnpkhuO/LOmHkp6X9OeSeitfanm6FzUCvhbdzPJn1kCXVAtsAx4CeoFNMwT2VyPipyPibuDzwBcqXmmZutuKge55dDPLm3JG6GuBwYg4GBGXgB3AhtIGEXG6ZLMVqNqC5N1txbtFHehmljd1ZbRZDhwu2R4C3jO9kaRPAZ8BGoAPzPRGkjYDmwFuvfXWa621LJdH6L500cxypmInRSNiW0TcDvxL4F9foc32iOiLiL5CoVCpj36LQjKH7itdzCxvygn0I0BPyfaKZN+V7AA+eiNF3Yim+lraGus85WJmuVNOoO8G1khaJakB2Aj0lzaQtKZk828BL1WuxGu3pK2BY2c95WJm+TLrHHpEjEvaAuwCaoHHI2KvpMeAgYjoB7ZI+hAwBowAn5jLomdTvP3fI3Qzy5dyTooSETuBndP2bS15/ekK13VDutsaODh8rtplmJnNq8zdKQrJCN1z6GaWM5kN9JHzY4xNTFa7FDOzeZPNQPft/2aWQ5kM9EJyc9GwT4yaWY5kMtBvbi8G+hunRqtciZnZ/MlkoC/taALgjdMOdDPLj0wGeqGtkRrBTxzoZpYjmQz0utoautsaPeViZrmSyUCH4rSLp1zMLE8yG+g3tzd5ysXMciWzgb60vclTLmaWK9kN9I4mTo+Oc+GSHxZtZvmQ2UC/ud2XLppZvmQ20JdNXYvuaRczy4nMBvrUCN0nRs0sLzIb6L5b1MzyJrOB3tZYR1tjnadczCw3MhvoUFyky4FuZnlRVqBLWifpgKRBSY/McPwzkvZJ2iPpO5Juq3yp1255ZwtHTl6odhlmZvNi1kCXVAtsAx4CeoFNknqnNXsO6IuIu4Angc9XutDrsaKzmaGR89Uuw8xsXpQzQl8LDEbEwYi4BOwANpQ2iIinI2IqOb8HrKhsmdenp7OFkfNjnL04Xu1SzMzmXDmBvhw4XLI9lOy7koeB/zXTAUmbJQ1IGhgeHi6/yuvU09UMwOETHqWbWfZV9KSopL8P9AG/MdPxiNgeEX0R0VcoFCr50TNa0dkCwNCI59HNLPvqymhzBOgp2V6R7HsLSR8Cfh34GxGxIB7m2dPpEbqZ5Uc5I/TdwBpJqyQ1ABuB/tIGku4Bvgysj4ijlS/z+nS1NtBcX+sRupnlwqyBHhHjwBZgF7AfeCIi9kp6TNL6pNlvAG3A1yU9L6n/Cm83ryTR09XMYV/pYmY5UM6UCxGxE9g5bd/WktcfqnBdFdPT2eIRupnlQqbvFIXkWvQT54mIapdiZjanMh/oPV0tnLk4zukLvhbdzLIt84G+IrnS5ce+0sXMMi7zgb6quw2AV46fq3IlZmZzK/OBftuSFiR4+ejZapdiZjanMh/oTfW19HS28PKwA93Msi3zgQ6wutDKwWFPuZhZtuUi0G8vtHHw2FkmJ33popllVy4CfXWhldGxSV7380XNLMNyEei3F4pXuvjEqJllWS4CfXWhFcAnRs0s03IR6IW2RhY11fnEqJllWi4CXRK3F9p46eiZapdiZjZnchHoAO9ctoj9r5/xIl1mllm5CfTeWzo4dWGMIye9lK6ZZVNuAv3OW9oB2Pva6SpXYmY2N3IT6HcsXYQE+xzoZpZRuQn0loY6Vne3eoRuZpmVm0AHuPOWDva9dqraZZiZzYmyAl3SOkkHJA1KemSG4++T9FeSxiV9rPJlVkbvLe28dmqUkXOXql2KmVnFzRrokmqBbcBDQC+wSVLvtGY/Bj4JfLXSBVbSu27pAOAFj9LNLIPKGaGvBQYj4mBEXAJ2ABtKG0TEoYjYA0zOQY0V8+6eDmoEA4dGql2KmVnFlRPoy4HDJdtDyb5rJmmzpAFJA8PDw9fzFjdkUVM971zWzu5DJ+b9s83M5tq8nhSNiO0R0RcRfYVCYT4/+rL7Vnbx3I9PMjaxoH+ZMDO7ZuUE+hGgp2R7RbIvle5b2cWFsQlfvmhmmVNOoO8G1khaJakB2Aj0z21Zc+e+lZ0ADHjaxcwyZtZAj4hxYAuwC9gPPBEReyU9Jmk9gKT7JA0BHwe+LGnvXBZ9I25qb+K2JS18/xUHupllS105jSJiJ7Bz2r6tJa93U5yKSYX3rl7Ct/a8ztjEJPW1ubq3yswyLJdp9oE7buLMxXFf7WJmmZLLQH/gHd001Nbw1P6j1S7FzKxichnorY113H/7Ep464EA3s+zIZaADfPCOmzg4fI5Xjvk5o2aWDbkN9A/13gzAt/a8VuVKzMwqI7eBvnxxM/ev7uLJZ4f8nFEzy4TcBjrAx36mh0PHz/Psq16sy8zSL9eB/tC7ltLSUMvXB4aqXYqZ2Q3LdaC3NtbxkbuW8Y0fHOH42YvVLsfM7IbkOtABNr9vNRfHJ/m9vzhU7VLMzG5I7gP9HTct4m/2LuUrf3GIM6Nj1S7HzOy65T7QAT71c+/g9Og4v/1nL1e7FDOz6+ZAB356RQd/597lbP8/B3l5+Gy1yzEzuy4O9MSjD72T5vpa/tUf/pCJSV+Xbmbp40BPFBY18m8+0sszr5zgN//0QLXLMTO7Zg70Eh/v62HjfT385+++zB8952vTzSxdynrARZ58dv2dvHr8PJ954geMTQS/1Ncz+18yM1sAPEKfpqm+lsc/eR8P3N7Nrz25h3/7jRcYHZuodllmZrNyoM+guaEY6v/wgVV85S9f5cEv/hnf3PMa4xOT1S7NzOyKygp0SeskHZA0KOmRGY43SvpacvwZSSsrXeh8a6irYesv9PLfHn4PzfW1bPnqc/z1zz/NF779InuGTjLpK2HMbIHRbEvHSqoFXgQeBIaA3cCmiNhX0uafAHdFxC9L2gj87Yj4u1d7376+vhgYGLjR+ufF+MQkT/3oKL//vVf588FjREBHcz3vXLaIO5a2c9uSFpa2N3FTexOdLfW0NtbR2lhHS30tNTWqdvlmliGSno2IvpmOlXNSdC0wGBEHkzfbAWwA9pW02QB8Nnn9JPCfJCkystB4XW0NH75zKR++cynHz17k/750jGdeOcH+10/ztd2HuXCVOfam+hrqa2qoqRF1NXrzT4naGlEjkN4e+lf8MTDDgSu1vab3NbN58ysfXMMvvPuWir9vOYG+HDhcsj0EvOdKbSJiXNIpYAlwrLSRpM3AZoBbb731OkuuriVtjXz0nuV89J7lAExOBifOX+Inp0f5yelRTl8Y59ylcc5dHOfcxQkujE0wPhFMRjA+OcnEJEyU/DnTzM2VfgrO9PPxij8xZ3zfTPx8NUu9jub6OXnfeb1sMSK2A9uhOOUyn589V2pqRHdbI91tjdx5S0e1yzGzHCvnpOgRoPRi7BXJvhnbSKoDOoDjlSjQzMzKU06g7wbWSFolqQHYCPRPa9MPfCJ5/THgqazMn5uZpcWsUy7JnPgWYBdQCzweEXslPQYMREQ/8F+A35c0CJygGPpmZjaPyppDj4idwM5p+7aWvB4FPl7Z0szM7Fr4TlEzs4xwoJuZZYQD3cwsIxzoZmYZMetaLnP2wdIw8Op1/vVupt2FmmJZ6UtW+gHuy0KUlX7AjffltogozHSgaoF+IyQNXGlxmrTJSl+y0g9wXxairPQD5rYvnnIxM8sIB7qZWUakNdC3V7uACspKX7LSD3BfFqKs9APmsC+pnEM3M7O3S+sI3czMpnGgm5llRKoCfbaHVS80kh6XdFTSCyX7uiR9W9JLyZ+dyX5J+o9J3/ZIurd6lb+dpB5JT0vaJ2mvpE8n+1PVH0lNkr4v6QdJP/5dsn9V8oDzweSB5w3J/gX/AHRJtZKek/TNZDuVfZF0SNIPJT0vaSDZl6rvLwBJiyU9KelHkvZLeu989SM1ga7iw6q3AQ8BvcAmSb3VrWpWvwesm7bvEeA7EbEG+E6yDcV+rUm+NgNfmqcayzUO/GpE9AL3A59K/vunrT8XgQ9ExLuBu4F1ku4HPgd8MSLeAYwADyftHwZGkv1fTNotNJ8G9pdsp7kvPxcRd5dcp5227y+A3wL+JCLuAN5N8f/N/PQjIlLxBbwX2FWy/SjwaLXrKqPulcALJdsHgGXJ62XAgeT1l4FNM7VbiF/AN4AH09wfoAX4K4rPyD0G1E3/XqP4HID3Jq/rknaqdu0lfViRBMQHgG9SfA54WvtyCOieti9V318Un9b2yvT/rvPVj9SM0Jn5YdXLq1TLjbg5Il5PXr8B3Jy8Tk3/kl/V7wGeIYX9SaYongeOAt8GXgZORsR40qS01rc8AB2YegD6QvEfgF8DJpPtJaS3LwH8qaRnkwfKQ/q+v1YBw8B/TabBfldSK/PUjzQFeuZE8Udyqq4bldQG/A/gn0XE6dJjaelPRExExN0UR7drgTuqXNJ1kfQR4GhEPFvtWirkZyPiXorTEJ+S9L7Sgyn5/qoD7gW+FBH3AOd4c3oFmNt+pCnQy3lYdRr8RNIygOTPo8n+Bd8/SfUUw/y/R8QfJrtT25+IOAk8TXFaYrGKDziHt9a6kB+A/gCwXtIhYAfFaZffIp19ISKOJH8eBf6I4g/btH1/DQFDEfFMsv0kxYCfl36kKdDLeVh1GpQ+UPsTFOeip/b/g+Ss9/3AqZJf0apOkig+O3Z/RHyh5FCq+iOpIGlx8rqZ4nmA/RSD/WNJs+n9WJAPQI+IRyNiRUSspPjv4amI+HuksC+SWiUtmnoNfBh4gZR9f0XEG8BhSX8t2fVBYB/z1Y9qn0S4xhMOPw+8SHHO89erXU8Z9f4B8DowRvEn98MU5yy/A7wE/G+gK2krilfxvAz8EOirdv3T+vKzFH9N3AM8n3z9fNr6A9wFPJf04wVga7J/NfB9YBD4OtCY7G9KtgeT46ur3Ycr9Ov9wDfT2pek5h8kX3un/n2n7fsrqe1uYCD5HvtjoHO++uFb/83MMiJNUy5mZnYVDnQzs4xwoJuZZYQD3cwsIxzoZmYZ4UA3M8sIB7qZWUb8f6x17sQC8hPhAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## MOMENTUM GRADIENT DESCENT OPTIMIZER\n",
        "\n",
        "import math\n",
        "\n",
        "# HyperParameters of the optimization algorithm\n",
        "alpha = 0.01\n",
        "beta = 0.9\n",
        "\n",
        "# Objective function\n",
        "def obj_func(x):\n",
        " return x * x - 4 * x + 4\n",
        " \n",
        "# Gradient of the objective function\n",
        "def grad(x):\n",
        " return 2 * x - 4\n",
        " \n",
        "# Parameter of the objective function\n",
        "x = 0\n",
        "iterations = 0\n",
        "\n",
        "v = random.random()\n",
        "k = []\n",
        "l2 = []\n",
        "while (1):\n",
        " iterations += 1\n",
        " v = beta * v + (1 - beta) * grad(x)\n",
        " loss = loss_function(v)\n",
        " x_prev = x\n",
        " \n",
        " x = x - alpha * v\n",
        " \n",
        " print('iteration '+str(iterations)+': weight='+str(x-1) + ':loss=' + str(loss))\n",
        " k.append(iterations)\n",
        " l2.append(loss)\n",
        " if x_prev == x:\n",
        "  print('converged after '+str(iterations)+' iterations')\n",
        "  break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErSeIuNIzAm2",
        "outputId": "fc0f7686-f548-4158-880f-d9fdc18e4539"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1: weight=-0.9980842261182926:loss=1.4198566719997956\n",
            "iteration 2: weight=-0.9923638611725194:loss=2.471298740282954\n",
            "iteration 3: weight=-0.9832308049989785:loss=3.6607383853987288\n",
            "iteration 4: weight=-0.9710445928327937:loss=4.922280102829664\n",
            "iteration 5: weight=-0.9561349126975618:loss=6.20492164439567\n",
            "iteration 6: weight=-0.938803930750458:loss=7.469825741929159\n",
            "iteration 7: weight=-0.9193284391365636:loss=8.688046058807572\n",
            "iteration 8: weight=-0.8979618398057855:loss=9.838635535775666\n",
            "iteration 9: weight=-0.8749359767284737:loss=10.907076320013484\n",
            "iteration 10: weight=-0.850462828005436:loss=11.883979828806682\n",
            "iteration 11: weight=-0.8247360684986914:loss=12.764013448527729\n",
            "iteration 12: weight=-0.7979325128056238:loss=13.545017116527294\n",
            "iteration 13: weight=-0.7702134476562517:loss=14.227278757425768\n",
            "iteration 14: weight=-0.7417258621265043:loss=14.812942399096206\n",
            "iteration 15: weight=-0.7126035834254786:loss=15.305526907607263\n",
            "iteration 16: weight=-0.6829683254277046:loss=15.70953676550112\n",
            "iteration 17: weight=-0.6529306565788524:loss=16.030149268503386\n",
            "iteration 18: weight=-0.6225908933017279:loss=16.27296501254447\n",
            "iteration 19: weight=-0.5920399245657124:loss=16.443810654293113\n",
            "iteration 20: weight=-0.5613599728541669:loss=16.548584712536673\n",
            "iteration 21: weight=-0.5306252963680678:loss=16.593138684271644\n",
            "iteration 22: weight=-0.4999028369378423:loss=16.583187020463555\n",
            "iteration 23: weight=-0.46925281777676375:loss=16.524240577960548\n",
            "iteration 24: weight=-0.43872929489623946:loss=16.421559066483713\n",
            "iteration 25: weight=-0.40838066571397513:loss=16.280118768878694\n",
            "iteration 26: weight=-0.37825013811850927:loss=16.10459245090443\n",
            "iteration 27: weight=-0.348376163006353:loss=15.899338912448561\n",
            "iteration 28: weight=-0.31879283307939965:loss=15.66840008106039\n",
            "iteration 29: weight=-0.28953025047898284:loss=15.415503924545517\n",
            "iteration 30: weight=-0.2606148656376498:loss=15.144071773490566\n",
            "iteration 31: weight=-0.23206978954917468:loss=14.857228906663329\n",
            "iteration 32: weight=-0.2039150814904488:loss=14.557817470466045\n",
            "iteration 33: weight=-0.17616801407461458:loss=14.248410984955338\n",
            "iteration 34: weight=-0.14884331737221457:loss=13.931329839261517\n",
            "iteration 35: weight=-0.12195340370531016:loss=13.608657303516642\n",
            "iteration 36: weight=-0.09550857459768558:loss=13.282255686839637\n",
            "iteration 37: weight=-0.06951721125162802:loss=12.95378235507934\n",
            "iteration 38: weight=-0.04398594981767301:loss=12.624705390880608\n",
            "iteration 39: weight=-0.018919842627478145:loss=12.296318734742375\n",
            "iteration 40: weight=0.005677493528952304:loss=11.969756691210428\n",
            "iteration 41: weight=0.02980374108268169:loss=11.646007720984226\n",
            "iteration 42: weight=0.053457756398872736:loss=11.325927469024258\n",
            "iteration 43: weight=0.07663945467064703:loss=11.010251001990664\n",
            "iteration 44: weight=0.09934970420590261:loss=10.699604246586848\n",
            "iteration 45: weight=0.12159022937922082:loss=10.394514634513628\n",
            "iteration 46: weight=0.14336352157644883:loss=10.095420970504243\n",
            "iteration 47: weight=0.16467275751080113:loss=9.802682547929326\n",
            "iteration 48: weight=0.18552172433669645:loss=9.51658754225195\n",
            "iteration 49: weight=0.20591475103132884:loss=9.237360716606418\n",
            "iteration 50: weight=0.2258566455544353:loss=8.965170476328389\n",
            "iteration 51: weight=0.24535263733412238:loss=8.700135310673653\n",
            "iteration 52: weight=0.26440832466117237:loss=8.442329660472993\n",
            "iteration 53: weight=0.28302962660619513:loss=8.191789250281621\n",
            "iteration 54: weight=0.30122273910350317:loss=7.948515922858685\n",
            "iteration 55: weight=0.31899409487287333:loss=7.712482012689348\n",
            "iteration 56: weight=0.33635032687556077:loss=7.483634293848629\n",
            "iteration 57: weight=0.3532982350242284:loss=7.261897535890265\n",
            "iteration 58: weight=0.3698447558879807:loss=7.047177699696412\n",
            "iteration 59: weight=0.3859969351535819:loss=6.839364803401411\n",
            "iteration 60: weight=0.4017619026223158:loss=6.638335486649169\n",
            "iteration 61: weight=0.41714684953893166:loss=6.443955299594066\n",
            "iteration 62: weight=0.4321590080648081:loss=6.256080741235733\n",
            "iteration 63: weight=0.4468056327219674:loss=6.074561069908866\n",
            "iteration 64: weight=0.4610939836479666:loss=5.8992399070449215\n",
            "iteration 65: weight=0.4750313115140701:loss=5.729956653693331\n",
            "iteration 66: weight=0.48862484397053496:loss=5.566547737742661\n",
            "iteration 67: weight=0.5018817734934125:loss=5.408847708320866\n",
            "iteration 68: weight=0.5148092465170153:loss=5.256690192480359\n",
            "iteration 69: weight=0.5274143537452238:loss=5.109908727988052\n",
            "iteration 70: weight=0.5397041215431211:loss=4.9683374848417605\n",
            "iteration 71: weight=0.5516855043181423:loss=4.831811887020014\n",
            "iteration 72: weight=0.5633653778070251:loss=4.700169144939655\n",
            "iteration 73: weight=0.5747505331914056:loss=4.57324870814099\n",
            "iteration 74: weight=0.5858476719709653:loss=4.450892646839998\n",
            "iteration 75: weight=0.5966634015286272:loss=4.332945970177134\n",
            "iteration 76: weight=0.6072042313274655:loss=4.219256888248446\n",
            "iteration 77: weight=0.617476569683765:loss=4.109677024322945\n",
            "iteration 78: weight=0.627486721065067:loss=4.004061583026254\n",
            "iteration 79: weight=0.6372408838661088:loss=3.9022694797006103\n",
            "iteration 80: weight=0.6467451486193141:loss=3.804163435631283\n",
            "iteration 81: weight=0.6560054965999602:loss=3.7096100433558146\n",
            "iteration 82: weight=0.6650277987893418:loss=3.618479805841537\n",
            "iteration 83: weight=0.6738178151622065:loss=3.530647152925271\n",
            "iteration 84: weight=0.6823811942674605:loss=3.445990438053772\n",
            "iteration 85: weight=0.690723473073654:loss=3.3643919180413864\n",
            "iteration 86: weight=0.698850077053081:loss=3.2857377182697602\n",
            "iteration 87: weight=0.7067663204804591:loss=3.209917785490679\n",
            "iteration 88: weight=0.7144774069241384:loss=3.1368258301548226\n",
            "iteration 89: weight=0.7219884299096015:loss=3.066359259974182\n",
            "iteration 90: weight=0.7293043737366991:loss=2.9984191062320007\n",
            "iteration 91: weight=0.7364301144336136:loss=2.932909944179521\n",
            "iteration 92: weight=0.7433704208319694:loss=2.8697398087017394\n",
            "iteration 93: weight=0.7501299557488257:loss=2.8088201062932443\n",
            "iteration 94: weight=0.7567132772624987:loss=2.750065524258494\n",
            "iteration 95: weight=0.7631248400702795:loss=2.693393937937305\n",
            "iteration 96: weight=0.7693689969171416:loss=2.638726316654557\n",
            "iteration 97: weight=0.7754500000854831:loss=2.585986629002118\n",
            "iteration 98: weight=0.7813720029368196:loss=2.5351017479796614\n",
            "iteration 99: weight=0.7871390614971487:loss=2.486001356448493\n",
            "iteration 100: weight=0.7927551360784506:loss=2.43861785328785\n",
            "iteration 101: weight=0.7982240929294655:loss=2.3928862605855947\n",
            "iteration 102: weight=0.80354970590952:loss=2.3487441321441316\n",
            "iteration 103: weight=0.80873565817975:loss=2.3061314635370236\n",
            "iteration 104: weight=0.8137855439065975:loss=2.264990603911669\n",
            "iteration 105: weight=0.8187028699729471:loss=2.2252661696979086\n",
            "iteration 106: weight=0.8234910576927157:loss=2.1869049603511703\n",
            "iteration 107: weight=0.828153444525122:loss=2.1498558762312387\n",
            "iteration 108: weight=0.8326932857852374:loss=2.1140698386935695\n",
            "iteration 109: weight=0.8371137563477709:loss=2.0794997124489436\n",
            "iteration 110: weight=0.8414179523413554:loss=2.046100230228812\n",
            "iteration 111: weight=0.8456088928308989:loss=2.013827919777626\n",
            "iteration 112: weight=0.8496895214858262:loss=1.9826410331795903\n",
            "iteration 113: weight=0.8536627082322892:loss=1.9524994785152625\n",
            "iteration 114: weight=0.8575312508876411:loss=1.9233647538331862\n",
            "iteration 115: weight=0.8612978767756827:loss=1.8951998834129555\n",
            "iteration 116: weight=0.8649652443213687:loss=1.8679693562887154\n",
            "iteration 117: weight=0.8685359446238434:loss=1.8416390669958642\n",
            "iteration 118: weight=0.8720125030068229:loss=1.8161762584985575\n",
            "iteration 119: weight=0.8753973805454909:loss=1.791549467251372\n",
            "iteration 120: weight=0.878692975569201:loss=1.767728470345062\n",
            "iteration 121: weight=0.8819016251394018:loss=1.7446842346836409\n",
            "iteration 122: weight=0.8850256065023037:loss=1.7223888681379536\n",
            "iteration 123: weight=0.8880671385159107:loss=1.7008155726193799\n",
            "iteration 124: weight=0.8910283830511252:loss=1.6799385990162872\n",
            "iteration 125: weight=0.8939114463667159:loss=1.6597332039352228\n",
            "iteration 126: weight=0.8967183804580143:loss=1.6401756081885908\n",
            "iteration 127: weight=0.8994511843792667:loss=1.6212429569706197\n",
            "iteration 128: weight=0.9021118055396353:loss=1.6029132816637515\n",
            "iteration 129: weight=0.9047021409728879:loss=1.5851654632181442\n",
            "iteration 130: weight=0.9072240385808694:loss=1.5679791970477306\n",
            "iteration 131: weight=0.909679298350891:loss=1.5513349593871901\n",
            "iteration 132: weight=0.9120696735472087:loss=1.535213975055243\n",
            "iteration 133: weight=0.9143968718768001:loss=1.5195981865708323\n",
            "iteration 134: weight=0.9166625566296789:loss=1.5044702245700203\n",
            "iteration 135: weight=0.9188683477940105:loss=1.4898133794727315\n",
            "iteration 136: weight=0.9210158231463208:loss=1.4756115743498717\n",
            "iteration 137: weight=0.9231065193171075:loss=1.4618493389427527\n",
            "iteration 138: weight=0.9251419328321813:loss=1.4485117847882063\n",
            "iteration 139: weight=0.9271235211300832:loss=1.4355845814042312\n",
            "iteration 140: weight=0.9290527035559348:loss=1.4230539334924832\n",
            "iteration 141: weight=0.9309308623320895:loss=1.4109065591153933\n",
            "iteration 142: weight=0.9327593435059645:loss=1.3991296688071508\n",
            "iteration 143: weight=0.93453945787544:loss=1.3877109455792462\n",
            "iteration 144: weight=0.9362724818922172:loss=1.3766385257826885\n",
            "iteration 145: weight=0.9379596585435321:loss=1.36590098079042\n",
            "iteration 146: weight=0.9396021982126286:loss=1.355487299464837\n",
            "iteration 147: weight=0.9412012795183902:loss=1.3453868713766632\n",
            "iteration 148: weight=0.9427580501345387:loss=1.3355894707427538\n",
            "iteration 149: weight=0.9442736275888033:loss=1.3260852410516832\n",
            "iteration 150: weight=0.9457491000424638:loss=1.3168646803472273\n",
            "iteration 151: weight=0.9471855270506735:loss=1.307918627141058\n",
            "iteration 152: weight=0.9485839403039609:loss=1.2992382469271526\n",
            "iteration 153: weight=0.9499453443513115:loss=1.2908150192715542\n",
            "iteration 154: weight=0.9512707173052244:loss=1.2826407254522292\n",
            "iteration 155: weight=0.9525610115291356:loss=1.2747074366248274\n",
            "iteration 156: weight=0.9538171543075975:loss=1.2670075024911804\n",
            "iteration 157: weight=0.955040048499598:loss=1.2595335404483716\n",
            "iteration 158: weight=0.9562305731753993:loss=1.252278425197155\n",
            "iteration 159: weight=0.9573895842372695:loss=1.2452352787894325\n",
            "iteration 160: weight=0.9585179150244783:loss=1.238397461095373\n",
            "iteration 161: weight=0.9596163769029171:loss=1.2317585606716117\n",
            "iteration 162: weight=0.9606857598397063:loss=1.2253123860127906\n",
            "iteration 163: weight=0.9617268329631372:loss=1.219052957169467\n",
            "iteration 164: weight=0.9627403451082988:loss=1.2129744977161934\n",
            "iteration 165: weight=0.9637270253487276:loss=1.207071427054271\n",
            "iteration 166: weight=0.964687583514416:loss=1.2013383530343873\n",
            "iteration 167: weight=0.9656227106965067:loss=1.1957700648849956\n",
            "iteration 168: weight=0.9665330797389953:loss=1.1903615264329457\n",
            "iteration 169: weight=0.9674193457177571:loss=1.1851078696034638\n",
            "iteration 170: weight=0.9682821464072071:loss=1.1800043881871747\n",
            "iteration 171: weight=0.9691221027348977:loss=1.1750465318624104\n",
            "iteration 172: weight=0.9699398192243496:loss=1.170229900461576\n",
            "iteration 173: weight=0.9707358844264076:loss=1.1655502384708611\n",
            "iteration 174: weight=0.971510871339407:loss=1.1610034297530665\n",
            "iteration 175: weight=0.9722653378184276:loss=1.1565854924837722\n",
            "iteration 176: weight=0.9729998269739093:loss=1.1522925742915342\n",
            "iteration 177: weight=0.973714867559895:loss=1.148120947593203\n",
            "iteration 178: weight=0.9744109743521623:loss=1.144067005115871\n",
            "iteration 179: weight=0.9750886485164985:loss=1.1401272555973412\n",
            "iteration 180: weight=0.9757483779673681:loss=1.1362983196573755\n",
            "iteration 181: weight=0.976390637717216:loss=1.1325769258323353\n",
            "iteration 182: weight=0.9770158902166448:loss=1.1289599067661613\n",
            "iteration 183: weight=0.9776245856856973:loss=1.1254441955509615\n",
            "iteration 184: weight=0.9782171624364733:loss=1.1220268222107819\n",
            "iteration 185: weight=0.9787940471872987:loss=1.1187049103224216\n",
            "iteration 186: weight=0.9793556553686669:loss=1.1154756737674438\n",
            "iteration 187: weight=0.9799023914211609:loss=1.112336413609783\n",
            "iteration 188: weight=0.9804346490855631:loss=1.1092845150936195\n",
            "iteration 189: weight=0.9809528116853541:loss=1.1063174447564186\n",
            "iteration 190: weight=0.9814572524017953:loss=1.1034327476522725\n",
            "iteration 191: weight=0.9819483345417888:loss=1.1006280446808996\n",
            "iteration 192: weight=0.9824264117986994:loss=1.0979010300178602\n",
            "iteration 193: weight=0.9828918285063215:loss=1.0952494686417564\n",
            "iteration 194: weight=0.9833449198861688:loss=1.0926711939543667\n",
            "iteration 195: weight=0.9837860122882589:loss=1.0901641054898525\n",
            "iteration 196: weight=0.9842154234255636:loss=1.0877261667093412\n",
            "iteration 197: weight=0.9846334626022866:loss=1.085355402877363\n",
            "iteration 198: weight=0.9850404309361327:loss=1.0830498990167698\n",
            "iteration 199: weight=0.9854366215747219:loss=1.080807797938916\n",
            "iteration 200: weight=0.9858223199063028:loss=1.0786272983460266\n",
            "iteration 201: weight=0.9861978037649131:loss=1.0765066530028122\n",
            "iteration 202: weight=0.9865633436301324:loss=1.0744441669745204\n",
            "iteration 203: weight=0.9869192028215696:loss=1.0724381959287372\n",
            "iteration 204: weight=0.9872656376882201:loss=1.070487144498373\n",
            "iteration 205: weight=0.9876028977928288:loss=1.068589464703376\n",
            "iteration 206: weight=0.9879312260913911:loss=1.0667436544288282\n",
            "iteration 207: weight=0.9882508591079144:loss=1.0649482559571755\n",
            "iteration 208: weight=0.9885620271045696:loss=1.0632018545524498\n",
            "iteration 209: weight=0.9888649542473502:loss=1.0615030770944271\n",
            "iteration 210: weight=0.989159858767358:loss=1.0598505907607558\n",
            "iteration 211: weight=0.9894469531178303:loss=1.058243101755179\n",
            "iteration 212: weight=0.9897264441270197:loss=1.056679354080048\n",
            "iteration 213: weight=0.989998533147036:loss=1.055158128351411\n",
            "iteration 214: weight=0.9902634161987567:loss=1.0536782406550227\n",
            "iteration 215: weight=0.9905212841129079:loss=1.0522385414417044\n",
            "iteration 216: weight=0.9907723226674181:loss=1.0508379144605386\n",
            "iteration 217: weight=0.9910167127211424:loss=1.0494752757284564\n",
            "iteration 218: weight=0.9912546303440521:loss=1.0481495725348289\n",
            "iteration 219: weight=0.9914862469439827:loss=1.0468597824797403\n",
            "iteration 220: weight=0.9917117293900322:loss=1.0456049125446676\n",
            "iteration 221: weight=0.9919312401326967:loss=1.044383998194351\n",
            "iteration 222: weight=0.9921449373208293:loss=1.043196102508689\n",
            "iteration 223: weight=0.9923529749155071:loss=1.0420403153435391\n",
            "iteration 224: weight=0.9925555028008861:loss=1.0409157525193493\n",
            "iteration 225: weight=0.9927526668921254:loss=1.0398215550365975\n",
            "iteration 226: weight=0.9929446092404566:loss=1.0387568883170484\n",
            "iteration 227: weight=0.9931314681354737:loss=1.0377209414698851\n",
            "iteration 228: weight=0.9933133782047181:loss=1.0367129265818091\n",
            "iteration 229: weight=0.9934904705106287:loss=1.0357320780302357\n",
            "iteration 230: weight=0.9936628726449268:loss=1.034777651818752\n",
            "iteration 231: weight=0.9938307088205054:loss=1.0338489249340392\n",
            "iteration 232: weight=0.9939940999608852:loss=1.0329451947234833\n",
            "iteration 233: weight=0.9941531637873051:loss=1.0320657782927434\n",
            "iteration 234: weight=0.9943080149035084:loss=1.031210011922563\n",
            "iteration 235: weight=0.9944587648782843:loss=1.0303772505041482\n",
            "iteration 236: weight=0.9946055223258261:loss=1.029566866992455\n",
            "iteration 237: weight=0.994748392983962:loss=1.02877825187676\n",
            "iteration 238: weight=0.9948874797903164:loss=1.0280108126679122\n",
            "iteration 239: weight=0.9950228829564549:loss=1.0272639734016804\n",
            "iteration 240: weight=0.9951547000400665:loss=1.0265371741576466\n",
            "iteration 241: weight=0.9952830260152368:loss=1.0258298705931026\n",
            "iteration 242: weight=0.9954079533408595:loss=1.0251415334914395\n",
            "iteration 243: weight=0.9955295720272384:loss=1.0244716483245315\n",
            "iteration 244: weight=0.9956479697009248:loss=1.023819714828638\n",
            "iteration 245: weight=0.9957632316678409:loss=1.0231852465933684\n",
            "iteration 246: weight=0.9958754409747295:loss=1.0225677706632637\n",
            "iteration 247: weight=0.9959846784689799:loss=1.0219668271515745\n",
            "iteration 248: weight=0.9960910228568673:loss=1.0213819688658257\n",
            "iteration 249: weight=0.9961945507602523:loss=1.0208127609447726\n",
            "iteration 250: weight=0.9962953367717782:loss=1.0202587805063739\n",
            "iteration 251: weight=0.9963934535086079:loss=1.0197196163064128\n",
            "iteration 252: weight=0.9964889716647374:loss=1.0191948684074172\n",
            "iteration 253: weight=0.9965819600619246:loss=1.0186841478575412\n",
            "iteration 254: weight=0.996672485699269:loss=1.0181870763790808\n",
            "iteration 255: weight=0.9967606138014806:loss=1.0177032860663096\n",
            "iteration 256: weight=0.9968464078658681:loss=1.0172324190923328\n",
            "iteration 257: weight=0.9969299297080851:loss=1.0167741274246684\n",
            "iteration 258: weight=0.9970112395066641:loss=1.0163280725492714\n",
            "iteration 259: weight=0.9970903958463719:loss=1.0158939252027333\n",
            "iteration 260: weight=0.9971674557604162:loss=1.0154713651123928\n",
            "iteration 261: weight=0.9972424747715354:loss=1.0150600807441073\n",
            "iteration 262: weight=0.9973155069319994:loss=1.0146597690574393\n",
            "iteration 263: weight=0.997386604862553:loss=1.0142701352680272\n",
            "iteration 264: weight=0.9974558197903263:loss=1.0138908926169088\n",
            "iteration 265: weight=0.9975232015857416:loss=1.0135217621465813\n",
            "iteration 266: weight=0.9975887987984438:loss=1.013162472483589\n",
            "iteration 267: weight=0.9976526586922789:loss=1.0128127596274303\n",
            "iteration 268: weight=0.9977148272793459:loss=1.0124723667455893\n",
            "iteration 269: weight=0.9977753493531476:loss=1.012141043974503\n",
            "iteration 270: weight=0.9978342685208628:loss=1.0118185482262811\n",
            "iteration 271: weight=0.9978916272347647:loss=1.0115046430009944\n",
            "iteration 272: weight=0.9979474668228068:loss=1.011199098204372\n",
            "iteration 273: weight=0.998001827518399:loss=1.0109016899707304\n",
            "iteration 274: weight=0.9980547484893953:loss=1.010612200490982\n",
            "iteration 275: weight=0.9981062678663133:loss=1.0103304178455652\n",
            "iteration 276: weight=0.9981564227698068:loss=1.0100561358421456\n",
            "iteration 277: weight=0.9982052493374114:loss=1.0097891538579489\n",
            "iteration 278: weight=0.9982527827495806:loss=1.0095292766865775\n",
            "iteration 279: weight=0.9982990572550339:loss=1.0092763143891847\n",
            "iteration 280: weight=0.9983441061954317:loss=1.009030082149868\n",
            "iteration 281: weight=0.9983879620293987:loss=1.0087904001351593\n",
            "iteration 282: weight=0.9984306563559104:loss=1.00855709335749\n",
            "iteration 283: weight=0.998472219937059:loss=1.0083299915425095\n",
            "iteration 284: weight=0.9985126827202186:loss=1.0081089290001441\n",
            "iteration 285: weight=0.9985520738596219:loss=1.007893744499288\n",
            "iteration 286: weight=0.9985904217373656:loss=1.0076842811460134\n",
            "iteration 287: weight=0.9986277539838602:loss=1.0074803862652022\n",
            "iteration 288: weight=0.9986640974977377:loss=1.0072819112854925\n",
            "iteration 289: weight=0.9986994784652319:loss=1.0070887116274478\n",
            "iteration 290: weight=0.9987339223790461:loss=1.0069006465948513\n",
            "iteration 291: weight=0.9987674540567208:loss=1.006717579269035\n",
            "iteration 292: weight=0.9988000976585147:loss=1.0065393764061548\n",
            "iteration 293: weight=0.9988318767048121:loss=1.0063659083373264\n",
            "iteration 294: weight=0.9988628140930702:loss=1.006197048871539\n",
            "iteration 295: weight=0.9988929321143163:loss=1.0060326752012647\n",
            "iteration 296: weight=0.9989222524692092:loss=1.0058726678106882\n",
            "iteration 297: weight=0.9989507962836743:loss=1.0057169103864785\n",
            "iteration 298: weight=0.9989785841241257:loss=1.0055652897310323\n",
            "iteration 299: weight=0.9990056360122836:loss=1.0054176956781156\n",
            "iteration 300: weight=0.9990319714396012:loss=1.0052740210108342\n",
            "iteration 301: weight=0.9990576093813077:loss=1.0051341613818718\n",
            "iteration 302: weight=0.999082568310081:loss=1.0049980152359221\n",
            "iteration 303: weight=0.999106866209357:loss=1.00486548373426\n",
            "iteration 304: weight=0.9991305205862866:loss=1.0047364706813877\n",
            "iteration 305: weight=0.9991535484843506:loss=1.0046108824536955\n",
            "iteration 306: weight=0.9991759664956394:loss=1.004488627930084\n",
            "iteration 307: weight=0.9991977907728082:loss=1.0043696184244877\n",
            "iteration 308: weight=0.9992190370407146:loss=1.0042537676202496\n",
            "iteration 309: weight=0.9992397206077488:loss=1.004140991506292\n",
            "iteration 310: weight=0.999259856376864:loss=1.0040312083150345\n",
            "iteration 311: weight=0.9992794588563141:loss=1.0039243384620107\n",
            "iteration 312: weight=0.9992985421701064:loss=1.0038203044871317\n",
            "iteration 313: weight=0.9993171200681794:loss=1.0037190309975563\n",
            "iteration 314: weight=0.9993352059363088:loss=1.0036204446121175\n",
            "iteration 315: weight=0.9993528128057525:loss=1.0035244739072644\n",
            "iteration 316: weight=0.9993699533626403:loss=1.0034310493644767\n",
            "iteration 317: weight=0.9993866399571141:loss=1.0033401033191105\n",
            "iteration 318: weight=0.9994028846122263:loss=1.0032515699106348\n",
            "iteration 319: weight=0.9994186990326028:loss=1.0031653850342217\n",
            "iteration 320: weight=0.9994340946128766:loss=1.0030814862936515\n",
            "iteration 321: weight=0.999449082445897:loss=1.0029998129554945\n",
            "iteration 322: weight=0.9994636733307236:loss=1.0029203059045386\n",
            "iteration 323: weight=0.9994778777804061:loss=1.002842907600423\n",
            "iteration 324: weight=0.9994917060295596:loss=1.0027675620354477\n",
            "iteration 325: weight=0.9995051680417386:loss=1.0026942146935263\n",
            "iteration 326: weight=0.9995182735166164:loss=1.0026228125102488\n",
            "iteration 327: weight=0.999531031896973:loss=1.0025533038340246\n",
            "iteration 328: weight=0.9995434523755:loss=1.0024856383882774\n",
            "iteration 329: weight=0.9995555439014234:loss=1.0024197672346598\n",
            "iteration 330: weight=0.9995673151869515:loss=1.0023556427372617\n",
            "iteration 331: weight=0.9995787747135529:loss=1.0022932185277875\n",
            "iteration 332: weight=0.9995899307380671:loss=1.0022324494716677\n",
            "iteration 333: weight=0.9996007912986538:loss=1.00217329163509\n",
            "iteration 334: weight=0.9996113642205844:loss=1.0021157022529148\n",
            "iteration 335: weight=0.9996216571218808:loss=1.002059639697457\n",
            "iteration 336: weight=0.9996316774188039:loss=1.0020050634481095\n",
            "iteration 337: weight=0.9996414323311971:loss=1.001951934061781\n",
            "iteration 338: weight=0.9996509288876885:loss=1.001900213144134\n",
            "iteration 339: weight=0.9996601739307553:loss=1.0018498633215915\n",
            "iteration 340: weight=0.9996691741216541:loss=1.0018008482141005\n",
            "iteration 341: weight=0.9996779359452197:loss=1.0017531324086248\n",
            "iteration 342: weight=0.9996864657145381:loss=1.001706681433351\n",
            "iteration 343: weight=0.9996947695754956:loss=1.0016614617325872\n",
            "iteration 344: weight=0.9997028535112065:loss=1.0016174406423346\n",
            "iteration 345: weight=0.9997107233463238:loss=1.001574586366517\n",
            "iteration 346: weight=0.9997183847512368:loss=1.0015328679538453\n",
            "iteration 347: weight=0.999725843246156:loss=1.0014922552753036\n",
            "iteration 348: weight=0.999733104205091:loss=1.0014527190022393\n",
            "iteration 349: weight=0.9997401728597222:loss=1.00141423058504\n",
            "iteration 350: weight=0.9997470543031708:loss=1.001376762232382\n",
            "iteration 351: weight=0.9997537534936682:loss=1.001340286891033\n",
            "iteration 352: weight=0.9997602752581287:loss=1.0013047782261995\n",
            "iteration 353: weight=0.9997666242956269:loss=1.0012702106023943\n",
            "iteration 354: weight=0.9997728051807839:loss=1.0012365590648231\n",
            "iteration 355: weight=0.9997788223670636:loss=1.0012037993212626\n",
            "iteration 356: weight=0.9997846801899812:loss=1.0011719077244277\n",
            "iteration 357: weight=0.9997903828702273:loss=1.0011408612548083\n",
            "iteration 358: weight=0.9997959345167082:loss=1.0011106375039651\n",
            "iteration 359: weight=0.9998013391295075:loss=1.0010812146582726\n",
            "iteration 360: weight=0.9998066006027679:loss=1.0010525714830953\n",
            "iteration 361: weight=0.9998117227274967:loss=1.0010246873073883\n",
            "iteration 362: weight=0.9998167091942978:loss=1.0009975420087067\n",
            "iteration 363: weight=0.9998215635960301:loss=1.0009711159986183\n",
            "iteration 364: weight=0.9998262894303971:loss=1.0009453902085035\n",
            "iteration 365: weight=0.9998308901024666:loss=1.000920346075735\n",
            "iteration 366: weight=0.9998353689271242:loss=1.0008959655302265\n",
            "iteration 367: weight=0.9998397291314618:loss=1.00087223098134\n",
            "iteration 368: weight=0.9998439738571028:loss=1.0008491253051421\n",
            "iteration 369: weight=0.9998481061624653:loss=1.0008266318320012\n",
            "iteration 370: weight=0.9998521290249667:loss=1.0008047343345134\n",
            "iteration 371: weight=0.999856045343168:loss=1.0007834170157535\n",
            "iteration 372: weight=0.9998598579388629:loss=1.000762664497836\n",
            "iteration 373: weight=0.9998635695591105:loss=1.0007424618107825\n",
            "iteration 374: weight=0.9998671828782151:loss=1.0007227943816859\n",
            "iteration 375: weight=0.9998707004996528:loss=1.0007036480241625\n",
            "iteration 376: weight=0.9998741249579475:loss=1.000685008928086\n",
            "iteration 377: weight=0.9998774587204968:loss=1.0006668636495943\n",
            "iteration 378: weight=0.9998807041893503:loss=1.0006491991013622\n",
            "iteration 379: weight=0.9998838637029397:loss=1.0006320025431343\n",
            "iteration 380: weight=0.9998869395377643:loss=1.0006152615725088\n",
            "iteration 381: weight=0.9998899339100309:loss=1.000598964115966\n",
            "iteration 382: weight=0.9998928489772507:loss=1.0005830984201385\n",
            "iteration 383: weight=0.9998956868397941:loss=1.0005676530433105\n",
            "iteration 384: weight=0.9998984495424035:loss=1.0005526168471446\n",
            "iteration 385: weight=0.9999011390756671:loss=1.000537978988629\n",
            "iteration 386: weight=0.9999037573774532:loss=1.0005237289122393\n",
            "iteration 387: weight=0.9999063063343057:loss=1.0005098563423063\n",
            "iteration 388: weight=0.9999087877828043:loss=1.0004963512755904\n",
            "iteration 389: weight=0.9999112035108875:loss=1.0004832039740514\n",
            "iteration 390: weight=0.9999135552591405:loss=1.0004704049578104\n",
            "iteration 391: weight=0.99991584472205:loss=1.0004579449982987\n",
            "iteration 392: weight=0.9999180735492244:loss=1.0004458151115907\n",
            "iteration 393: weight=0.9999202433465828:loss=1.0004340065519126\n",
            "iteration 394: weight=0.9999223556775123:loss=1.0004225108053226\n",
            "iteration 395: weight=0.9999244120639939:loss=1.0004113195835613\n",
            "iteration 396: weight=0.9999264139876993:loss=1.0004004248180647\n",
            "iteration 397: weight=0.9999283628910587:loss=1.000389818654135\n",
            "iteration 398: weight=0.9999302601783:loss=1.0003794934452679\n",
            "iteration 399: weight=0.9999321072164606:loss=1.000369441747631\n",
            "iteration 400: weight=0.9999339053363723:loss=1.000359656314686\n",
            "iteration 401: weight=0.9999356558336201:loss=1.0003501300919577\n",
            "iteration 402: weight=0.9999373599694759:loss=1.0003408562119385\n",
            "iteration 403: weight=0.9999390189718071:loss=1.0003318279891305\n",
            "iteration 404: weight=0.9999406340359616:loss=1.0003230389152182\n",
            "iteration 405: weight=0.9999422063256287:loss=1.0003144826543697\n",
            "iteration 406: weight=0.9999437369736779:loss=1.0003061530386625\n",
            "iteration 407: weight=0.9999452270829747:loss=1.0002980440636313\n",
            "iteration 408: weight=0.9999466777271759:loss=1.0002901498839327\n",
            "iteration 409: weight=0.9999480899515027:loss=1.0002824648091273\n",
            "iteration 410: weight=0.9999494647734939:loss=1.0002749832995705\n",
            "iteration 411: weight=0.999950803183739:loss=1.0002676999624163\n",
            "iteration 412: weight=0.9999521061465919:loss=1.0002606095477231\n",
            "iteration 413: weight=0.9999533746008664:loss=1.0002537069446669\n",
            "iteration 414: weight=0.9999546094605118:loss=1.0002469871778508\n",
            "iteration 415: weight=0.9999558116152716:loss=1.0002404454037166\n",
            "iteration 416: weight=0.9999569819313248:loss=1.0002340769070484\n",
            "iteration 417: weight=0.99995812125191:loss=1.0002278770975706\n",
            "iteration 418: weight=0.999959230397933:loss=1.000221841506636\n",
            "iteration 419: weight=0.9999603101685577:loss=1.000215965784001\n",
            "iteration 420: weight=0.9999613613417828:loss=1.000210245694688\n",
            "iteration 421: weight=0.9999623846750019:loss=1.0002046771159285\n",
            "iteration 422: weight=0.999963380905549:loss=1.00019925603419\n",
            "iteration 423: weight=0.9999643507512304:loss=1.0001939785422802\n",
            "iteration 424: weight=0.9999652949108411:loss=1.0001888408365278\n",
            "iteration 425: weight=0.999966214064669:loss=1.0001838392140399\n",
            "iteration 426: weight=0.9999671088749849:loss=1.0001789700700294\n",
            "iteration 427: weight=0.9999679799865191:loss=1.000174229895216\n",
            "iteration 428: weight=0.999968828026927:loss=1.0001696152732944\n",
            "iteration 429: weight=0.9999696536072402:loss=1.00016512287847\n",
            "iteration 430: weight=0.9999704573223076:loss=1.0001607494730602\n",
            "iteration 431: weight=0.9999712397512237:loss=1.0001564919051602\n",
            "iteration 432: weight=0.9999720014577458:loss=1.0001523471063678\n",
            "iteration 433: weight=0.9999727429907002:loss=1.0001483120895724\n",
            "iteration 434: weight=0.9999734648843777:loss=1.0001443839467998\n",
            "iteration 435: weight=0.9999741676589187:loss=1.0001405598471151\n",
            "iteration 436: weight=0.9999748518206877:loss=1.000136837034581\n",
            "iteration 437: weight=0.9999755178626384:loss=1.0001332128262705\n",
            "iteration 438: weight=0.9999761662646689:loss=1.0001296846103331\n",
            "iteration 439: weight=0.9999767974939668:loss=1.00012624984411\n",
            "iteration 440: weight=0.9999774120053471:loss=1.0001229060523007\n",
            "iteration 441: weight=0.9999780102415787:loss=1.0001196508251795\n",
            "iteration 442: weight=0.9999785926337039:loss=1.0001164818168566\n",
            "iteration 443: weight=0.9999791596013492:loss=1.0001133967435871\n",
            "iteration 444: weight=0.9999797115530273:loss=1.0001103933821245\n",
            "iteration 445: weight=0.9999802488864316:loss=1.0001074695681171\n",
            "iteration 446: weight=0.9999807719887226:loss=1.0001046231945483\n",
            "iteration 447: weight=0.999981281236807:loss=1.0001018522102163\n",
            "iteration 448: weight=0.9999817769976094:loss=1.0000991546182572\n",
            "iteration 449: weight=0.9999822596283363:loss=1.000096528474703\n",
            "iteration 450: weight=0.9999827294767338:loss=1.0000939718870816\n",
            "iteration 451: weight=0.9999831868813382:loss=1.0000914830130518\n",
            "iteration 452: weight=0.9999836321717195:loss=1.000089060059076\n",
            "iteration 453: weight=0.9999840656687191:loss=1.0000867012791252\n",
            "iteration 454: weight=0.9999844876846813:loss=1.000084404973423\n",
            "iteration 455: weight=0.9999848985236779:loss=1.0000821694872177\n",
            "iteration 456: weight=0.9999852984817275:loss=1.0000799932095912\n",
            "iteration 457: weight=0.9999856878470086:loss=1.0000778745722962\n",
            "iteration 458: weight=0.9999860669000677:loss=1.0000758120486275\n",
            "iteration 459: weight=0.9999864359140207:loss=1.0000738041523196\n",
            "iteration 460: weight=0.9999867951547503:loss=1.0000718494364766\n",
            "iteration 461: weight=0.9999871448810975:loss=1.0000699464925291\n",
            "iteration 462: weight=0.9999874853450479:loss=1.0000680939492175\n",
            "iteration 463: weight=0.9999878167919132:loss=1.0000662904716056\n",
            "iteration 464: weight=0.9999881394605079:loss=1.0000645347601167\n",
            "iteration 465: weight=0.9999884535833223:loss=1.000062825549598\n",
            "iteration 466: weight=0.9999887593866885:loss=1.000061161608408\n",
            "iteration 467: weight=0.9999890570909449:loss=1.0000595417375289\n",
            "iteration 468: weight=0.9999893469105936:loss=1.0000579647697019\n",
            "iteration 469: weight=0.9999896290544563:loss=1.000056429568587\n",
            "iteration 470: weight=0.9999899037258237:loss=1.0000549350279428\n",
            "iteration 471: weight=0.9999901711226029:loss=1.0000534800708303\n",
            "iteration 472: weight=0.9999904314374588:loss=1.000052063648835\n",
            "iteration 473: weight=0.9999906848579543:loss=1.000050684741313\n",
            "iteration 474: weight=0.9999909315666844:loss=1.0000493423546544\n",
            "iteration 475: weight=0.9999911717414081:loss=1.0000480355215675\n",
            "iteration 476: weight=0.9999914055551766:loss=1.0000467633003811\n",
            "iteration 477: weight=0.9999916331764578:loss=1.000045524774367\n",
            "iteration 478: weight=0.9999918547692581:loss=1.0000443190510777\n",
            "iteration 479: weight=0.9999920704932399:loss=1.000043145261705\n",
            "iteration 480: weight=0.999992280503837:loss=1.0000420025604513\n",
            "iteration 481: weight=0.9999924849523667:loss=1.0000408901239233\n",
            "iteration 482: weight=0.9999926839861386:loss=1.0000398071505359\n",
            "iteration 483: weight=0.9999928777485612:loss=1.0000387528599357\n",
            "iteration 484: weight=0.9999930663792442:loss=1.000037726492438\n",
            "iteration 485: weight=0.9999932500141004:loss=1.0000367273084805\n",
            "iteration 486: weight=0.9999934287854428:loss=1.0000357545880882\n",
            "iteration 487: weight=0.9999936028220802:loss=1.0000348076303571\n",
            "iteration 488: weight=0.9999937722494097:loss=1.0000338857529467\n",
            "iteration 489: weight=0.9999939371895072:loss=1.00003298829159\n",
            "iteration 490: weight=0.9999940977612161:loss=1.0000321145996136\n",
            "iteration 491: weight=0.9999942540802316:loss=1.000031264047473\n",
            "iteration 492: weight=0.9999944062591852:loss=1.0000304360222965\n",
            "iteration 493: weight=0.9999945544077251:loss=1.0000296299274467\n",
            "iteration 494: weight=0.9999946986325956:loss=1.0000288451820882\n",
            "iteration 495: weight=0.9999948390377138:loss=1.0000280812207698\n",
            "iteration 496: weight=0.9999949757242448:loss=1.000027337493017\n",
            "iteration 497: weight=0.9999951087906742:loss=1.0000266134629354\n",
            "iteration 498: weight=0.9999952383328792:loss=1.000025908608824\n",
            "iteration 499: weight=0.9999953644441979:loss=1.0000252224228\n",
            "iteration 500: weight=0.9999954872154964:loss=1.000024554410432\n",
            "iteration 501: weight=0.999995606735234:loss=1.0000239040903849\n",
            "iteration 502: weight=0.9999957230895276:loss=1.0000232709940713\n",
            "iteration 503: weight=0.9999958363622126:loss=1.0000226546653153\n",
            "iteration 504: weight=0.9999959466349047:loss=1.000022054660023\n",
            "iteration 505: weight=0.9999960539870578:loss=1.000021470545863\n",
            "iteration 506: weight=0.9999961584960215:loss=1.0000209019019546\n",
            "iteration 507: weight=0.9999962602370969:loss=1.000020348318564\n",
            "iteration 508: weight=0.9999963592835903:loss=1.0000198093968096\n",
            "iteration 509: weight=0.9999964557068672:loss=1.000019284748375\n",
            "iteration 510: weight=0.9999965495764027:loss=1.0000187739952284\n",
            "iteration 511: weight=0.999996640959832:loss=1.0000182767693504\n",
            "iteration 512: weight=0.9999967299229986:loss=1.0000177927124687\n",
            "iteration 513: weight=0.9999968165300026:loss=1.0000173214758001\n",
            "iteration 514: weight=0.9999969008432461:loss=1.0000168627197994\n",
            "iteration 515: weight=0.9999969829234787:loss=1.000016416113914\n",
            "iteration 516: weight=0.9999970628298411:loss=1.000015981336347\n",
            "iteration 517: weight=0.9999971406199077:loss=1.0000155580738235\n",
            "iteration 518: weight=0.9999972163497279:loss=1.0000151460213664\n",
            "iteration 519: weight=0.9999972900738665:loss=1.0000147448820762\n",
            "iteration 520: weight=0.9999973618454436:loss=1.0000143543669162\n",
            "iteration 521: weight=0.9999974317161722:loss=1.000013974194506\n",
            "iteration 522: weight=0.9999974997363954:loss=1.0000136040909167\n",
            "iteration 523: weight=0.9999975659551235:loss=1.0000132437894753\n",
            "iteration 524: weight=0.9999976304200686:loss=1.0000128930305714\n",
            "iteration 525: weight=0.9999976931776791:loss=1.0000125515614704\n",
            "iteration 526: weight=0.9999977542731731:loss=1.0000122191361318\n",
            "iteration 527: weight=0.9999978137505714:loss=1.000011895515031\n",
            "iteration 528: weight=0.9999978716527287:loss=1.0000115804649878\n",
            "iteration 529: weight=0.9999979280213649:loss=1.000011273758998\n",
            "iteration 530: weight=0.9999979828970946:loss=1.0000109751760689\n",
            "iteration 531: weight=0.9999980363194572:loss=1.0000106845010615\n",
            "iteration 532: weight=0.9999980883269446:loss=1.0000104015245348\n",
            "iteration 533: weight=0.9999981389570294:loss=1.0000101260425944\n",
            "iteration 534: weight=0.9999981882461917:loss=1.000009857856747\n",
            "iteration 535: weight=0.9999982362299453:loss=1.000009596773755\n",
            "iteration 536: weight=0.9999982829428637:loss=1.0000093426055003\n",
            "iteration 537: weight=0.9999983284186045:loss=1.0000090951688465\n",
            "iteration 538: weight=0.999998372689934:loss=1.0000088542855072\n",
            "iteration 539: weight=0.9999984157887507:loss=1.0000086197819185\n",
            "iteration 540: weight=0.9999984577461083:loss=1.000008391489113\n",
            "iteration 541: weight=0.9999984985922379:loss=1.0000081692425986\n",
            "iteration 542: weight=0.9999985383565699:loss=1.0000079528822399\n",
            "iteration 543: weight=0.9999985770677557:loss=1.0000077422521427\n",
            "iteration 544: weight=0.9999986147536875:loss=1.0000075372005415\n",
            "iteration 545: weight=0.9999986514415187:loss=1.0000073375796903\n",
            "iteration 546: weight=0.9999986871576836:loss=1.0000071432457562\n",
            "iteration 547: weight=0.9999987219279167:loss=1.000006954058716\n",
            "iteration 548: weight=0.9999987557772707:loss=1.0000067698822548\n",
            "iteration 549: weight=0.9999987887301347:loss=1.000006590583668\n",
            "iteration 550: weight=0.9999988208102522:loss=1.0000064160337656\n",
            "iteration 551: weight=0.9999988520407372:loss=1.0000062461067794\n",
            "iteration 552: weight=0.9999988824440924:loss=1.0000060806802722\n",
            "iteration 553: weight=0.9999989120422239:loss=1.000005919635049\n",
            "iteration 554: weight=0.9999989408564578:loss=1.000005762855073\n",
            "iteration 555: weight=0.9999989689075552:loss=1.0000056102273787\n",
            "iteration 556: weight=0.9999989962157279:loss=1.0000054616419944\n",
            "iteration 557: weight=0.9999990228006519:loss=1.0000053169918597\n",
            "iteration 558: weight=0.9999990486814823:loss=1.0000051761727504\n",
            "iteration 559: weight=0.9999990738768665:loss=1.000005039083202\n",
            "iteration 560: weight=0.9999990984049585:loss=1.0000049056244382\n",
            "iteration 561: weight=0.9999991222834315:loss=1.0000047757002983\n",
            "iteration 562: weight=0.9999991455294903:loss=1.000004649217168\n",
            "iteration 563: weight=0.9999991681598843:loss=1.000004526083913\n",
            "iteration 564: weight=0.9999991901909191:loss=1.0000044062118125\n",
            "iteration 565: weight=0.9999992116384686:loss=1.0000042895144952\n",
            "iteration 566: weight=0.9999992325179863:loss=1.000004175907878\n",
            "iteration 567: weight=0.9999992528445161:loss=1.0000040653101037\n",
            "iteration 568: weight=0.9999992726327038:loss=1.000003957641484\n",
            "iteration 569: weight=0.9999992918968075:loss=1.000003852824441\n",
            "iteration 570: weight=0.9999993106507072:loss=1.0000037507834512\n",
            "iteration 571: weight=0.9999993289079154:loss=1.000003651444991\n",
            "iteration 572: weight=0.9999993466815871:loss=1.0000035547374848\n",
            "iteration 573: weight=0.9999993639845284:loss=1.0000034605912522\n",
            "iteration 574: weight=0.9999993808292065:loss=1.0000033689384586\n",
            "iteration 575: weight=0.9999993972277583:loss=1.0000032797130656\n",
            "iteration 576: weight=0.9999994131919996:loss=1.000003192850784\n",
            "iteration 577: weight=0.9999994287334326:loss=1.0000031082890275\n",
            "iteration 578: weight=0.9999994438632556:loss=1.0000030259668669\n",
            "iteration 579: weight=0.9999994585923697:loss=1.0000029458249873\n",
            "iteration 580: weight=0.9999994729313877:loss=1.0000028678056443\n",
            "iteration 581: weight=0.999999486890641:loss=1.0000027918526229\n",
            "iteration 582: weight=0.9999995004801878:loss=1.0000027179111972\n",
            "iteration 583: weight=0.9999995137098194:loss=1.0000026459280904\n",
            "iteration 584: weight=0.9999995265890684:loss=1.0000025758514373\n",
            "iteration 585: weight=0.9999995391272143:loss=1.0000025076307453\n",
            "iteration 586: weight=0.9999995513332911:loss=1.00000244121686\n",
            "iteration 587: weight=0.9999995632160936:loss=1.0000023765619288\n",
            "iteration 588: weight=0.9999995747841837:loss=1.0000023136193659\n",
            "iteration 589: weight=0.9999995860458963:loss=1.0000022523438197\n",
            "iteration 590: weight=0.999999597009346:loss=1.0000021926911398\n",
            "iteration 591: weight=0.999999607682432:loss=1.0000021346183448\n",
            "iteration 592: weight=0.9999996180728445:loss=1.0000020780835919\n",
            "iteration 593: weight=0.9999996281880701:loss=1.0000020230461464\n",
            "iteration 594: weight=0.999999638035397:loss=1.0000019694663524\n",
            "iteration 595: weight=0.9999996476219204:loss=1.0000019173056047\n",
            "iteration 596: weight=0.9999996569545477:loss=1.00000186652632\n",
            "iteration 597: weight=0.9999996660400032:loss=1.0000018170919105\n",
            "iteration 598: weight=0.9999996748848332:loss=1.0000017689667575\n",
            "iteration 599: weight=0.9999996834954104:loss=1.0000017221161859\n",
            "iteration 600: weight=0.999999691877939:loss=1.0000016765064386\n",
            "iteration 601: weight=0.9999997000384591:loss=1.0000016321046525\n",
            "iteration 602: weight=0.99999970798285:loss=1.0000015888788354\n",
            "iteration 603: weight=0.9999997157168361:loss=1.000001546797842\n",
            "iteration 604: weight=0.9999997232459901:loss=1.000001505831352\n",
            "iteration 605: weight=0.9999997305757367:loss=1.0000014659498477\n",
            "iteration 606: weight=0.999999737711357:loss=1.000001427124594\n",
            "iteration 607: weight=0.9999997446579927:loss=1.000001389327616\n",
            "iteration 608: weight=0.9999997514206489:loss=1.0000013525316804\n",
            "iteration 609: weight=0.9999997580041982:loss=1.0000013167102746\n",
            "iteration 610: weight=0.999999764413384:loss=1.0000012818375887\n",
            "iteration 611: weight=0.9999997706528245:loss=1.0000012478884959\n",
            "iteration 612: weight=0.9999997767270155:loss=1.0000012148385349\n",
            "iteration 613: weight=0.9999997826403331:loss=1.0000011826638928\n",
            "iteration 614: weight=0.9999997883970384:loss=1.0000011513413871\n",
            "iteration 615: weight=0.999999794001279:loss=1.0000011208484487\n",
            "iteration 616: weight=0.999999799457093:loss=1.0000010911631074\n",
            "iteration 617: weight=0.9999998047684115:loss=1.0000010622639737\n",
            "iteration 618: weight=0.9999998099390612:loss=1.000001034130225\n",
            "iteration 619: weight=0.999999814972768:loss=1.0000010067415908\n",
            "iteration 620: weight=0.9999998198731586:loss=1.0000009800783367\n",
            "iteration 621: weight=0.9999998246437636:loss=1.0000009541212511\n",
            "iteration 622: weight=0.9999998292880206:loss=1.0000009288516314\n",
            "iteration 623: weight=0.9999998338092759:loss=1.0000009042512703\n",
            "iteration 624: weight=0.9999998382107871:loss=1.0000008803024427\n",
            "iteration 625: weight=0.9999998424957257:loss=1.0000008569878929\n",
            "iteration 626: weight=0.9999998466671789:loss=1.000000834290822\n",
            "iteration 627: weight=0.9999998507281525:loss=1.0000008121948765\n",
            "iteration 628: weight=0.9999998546815723:loss=1.0000007906841357\n",
            "iteration 629: weight=0.999999858530287:loss=1.0000007697431008\n",
            "iteration 630: weight=0.9999998622770696:loss=1.000000749356683\n",
            "iteration 631: weight=0.9999998659246199:loss=1.0000007295101936\n",
            "iteration 632: weight=0.9999998694755659:loss=1.0000007101893327\n",
            "iteration 633: weight=0.9999998729324662:loss=1.000000691380179\n",
            "iteration 634: weight=0.9999998762978115:loss=1.0000006730691804\n",
            "iteration 635: weight=0.9999998795740266:loss=1.000000655243143\n",
            "iteration 636: weight=0.9999998827634722:loss=1.0000006378892232\n",
            "iteration 637: weight=0.9999998858684462:loss=1.000000620994917\n",
            "iteration 638: weight=0.9999998888911861:loss=1.0000006045480514\n",
            "iteration 639: weight=0.9999998918338695:loss=1.0000005885367762\n",
            "iteration 640: weight=0.9999998946986168:loss=1.000000572949555\n",
            "iteration 641: weight=0.9999998974874922:loss=1.0000005577751565\n",
            "iteration 642: weight=0.9999999002025051:loss=1.0000005430026477\n",
            "iteration 643: weight=0.9999999028456117:loss=1.0000005286213844\n",
            "iteration 644: weight=0.9999999054187163:loss=1.0000005146210047\n",
            "iteration 645: weight=0.999999907923673:loss=1.0000005009914208\n",
            "iteration 646: weight=0.9999999103622867:loss=1.0000004877228126\n",
            "iteration 647: weight=0.9999999127363146:loss=1.0000004748056195\n",
            "iteration 648: weight=0.999999915047467:loss=1.0000004622305343\n",
            "iteration 649: weight=0.9999999172974092:loss=1.0000004499884967\n",
            "iteration 650: weight=0.9999999194877625:loss=1.0000004380706857\n",
            "iteration 651: weight=0.9999999216201048:loss=1.0000004264685145\n",
            "iteration 652: weight=0.9999999236959727:loss=1.0000004151736233\n",
            "iteration 653: weight=0.9999999257168619:loss=1.0000004041778738\n",
            "iteration 654: weight=0.9999999276842284:loss=1.0000003934733437\n",
            "iteration 655: weight=0.9999999295994899:loss=1.0000003830523199\n",
            "iteration 656: weight=0.9999999314640262:loss=1.0000003729072937\n",
            "iteration 657: weight=0.9999999332791809:loss=1.0000003630309555\n",
            "iteration 658: weight=0.9999999350462616:loss=1.0000003534161892\n",
            "iteration 659: weight=0.9999999367665418:loss=1.000000344056067\n",
            "iteration 660: weight=0.9999999384412608:loss=1.000000334943845\n",
            "iteration 661: weight=0.9999999400716255:loss=1.0000003260729575\n",
            "iteration 662: weight=0.9999999416588103:loss=1.000000317437013\n",
            "iteration 663: weight=0.9999999432039592:loss=1.0000003090297886\n",
            "iteration 664: weight=0.9999999447081853:loss=1.0000003008452272\n",
            "iteration 665: weight=0.9999999461725724:loss=1.0000002928774314\n",
            "iteration 666: weight=0.9999999475981756:loss=1.0000002851206604\n",
            "iteration 667: weight=0.9999999489860221:loss=1.000000277569325\n",
            "iteration 668: weight=0.999999950337112:loss=1.0000002702179847\n",
            "iteration 669: weight=0.9999999516524185:loss=1.0000002630613423\n",
            "iteration 670: weight=0.9999999529328896:loss=1.0000002560942416\n",
            "iteration 671: weight=0.9999999541794478:loss=1.0000002493116622\n",
            "iteration 672: weight=0.9999999553929912:loss=1.0000002427087178\n",
            "iteration 673: weight=0.9999999565743944:loss=1.0000002362806502\n",
            "iteration 674: weight=0.9999999577245084:loss=1.0000002300228281\n",
            "iteration 675: weight=0.9999999588441622:loss=1.0000002239307424\n",
            "iteration 676: weight=0.999999959934162:loss=1.000000218000004\n",
            "iteration 677: weight=0.9999999609952936:loss=1.0000002122263394\n",
            "iteration 678: weight=0.9999999620283215:loss=1.0000002066055884\n",
            "iteration 679: weight=0.99999996303399:loss=1.0000002011337015\n",
            "iteration 680: weight=0.9999999640130235:loss=1.000000195806736\n",
            "iteration 681: weight=0.9999999649661278:loss=1.0000001906208533\n",
            "iteration 682: weight=0.9999999658939893:loss=1.0000001855723173\n",
            "iteration 683: weight=0.9999999667972768:loss=1.0000001806574903\n",
            "iteration 684: weight=0.9999999676766409:loss=1.000000175872831\n",
            "iteration 685: weight=0.9999999685327152:loss=1.0000001712148918\n",
            "iteration 686: weight=0.9999999693661168:loss=1.0000001666803169\n",
            "iteration 687: weight=0.999999970177446:loss=1.0000001622658388\n",
            "iteration 688: weight=0.9999999709672873:loss=1.000000157968277\n",
            "iteration 689: weight=0.99999997173621:loss=1.0000001537845347\n",
            "iteration 690: weight=0.9999999724847679:loss=1.0000001497115973\n",
            "iteration 691: weight=0.9999999732135005:loss=1.0000001457465308\n",
            "iteration 692: weight=0.9999999739229328:loss=1.0000001418864777\n",
            "iteration 693: weight=0.9999999746135761:loss=1.000000138128657\n",
            "iteration 694: weight=0.9999999752859279:loss=1.000000134470361\n",
            "iteration 695: weight=0.9999999759404725:loss=1.0000001309089541\n",
            "iteration 696: weight=0.9999999765776819:loss=1.00000012744187\n",
            "iteration 697: weight=0.999999977198015:loss=1.0000001240666103\n",
            "iteration 698: weight=0.9999999778019186:loss=1.0000001207807434\n",
            "iteration 699: weight=0.9999999783898281:loss=1.0000001175819018\n",
            "iteration 700: weight=0.999999978962167:loss=1.0000001144677806\n",
            "iteration 701: weight=0.9999999795193477:loss=1.0000001114361359\n",
            "iteration 702: weight=0.9999999800617716:loss=1.0000001084847834\n",
            "iteration 703: weight=0.9999999805898296:loss=1.0000001056115966\n",
            "iteration 704: weight=0.9999999811039022:loss=1.0000001028145051\n",
            "iteration 705: weight=0.9999999816043597:loss=1.0000001000914938\n",
            "iteration 706: weight=0.9999999820915626:loss=1.0000000974406007\n",
            "iteration 707: weight=0.9999999825658621:loss=1.0000000948599157\n",
            "iteration 708: weight=0.9999999830276001:loss=1.0000000923475794\n",
            "iteration 709: weight=0.999999983477109:loss=1.0000000899017816\n",
            "iteration 710: weight=0.9999999839147127:loss=1.00000008752076\n",
            "iteration 711: weight=0.9999999843407266:loss=1.000000085202799\n",
            "iteration 712: weight=0.9999999847554577:loss=1.0000000829462286\n",
            "iteration 713: weight=0.9999999851592047:loss=1.0000000807494227\n",
            "iteration 714: weight=0.9999999855522588:loss=1.0000000786107985\n",
            "iteration 715: weight=0.9999999859349029:loss=1.0000000765288153\n",
            "iteration 716: weight=0.9999999863074127:loss=1.0000000745019726\n",
            "iteration 717: weight=0.9999999866700569:loss=1.0000000725288103\n",
            "iteration 718: weight=0.9999999870230964:loss=1.0000000706079066\n",
            "iteration 719: weight=0.9999999873667857:loss=1.0000000687378774\n",
            "iteration 720: weight=0.9999999877013726:loss=1.0000000669173754\n",
            "iteration 721: weight=0.999999988027098:loss=1.0000000651450889\n",
            "iteration 722: weight=0.9999999883441968:loss=1.000000063419741\n",
            "iteration 723: weight=0.9999999886528972:loss=1.0000000617400882\n",
            "iteration 724: weight=0.9999999889534217:loss=1.0000000601049206\n",
            "iteration 725: weight=0.999999989245987:loss=1.0000000585130597\n",
            "iteration 726: weight=0.9999999895308038:loss=1.0000000569633591\n",
            "iteration 727: weight=0.9999999898080774:loss=1.0000000554547017\n",
            "iteration 728: weight=0.9999999900780074:loss=1.0000000539860006\n",
            "iteration 729: weight=0.9999999903407883:loss=1.0000000525561976\n",
            "iteration 730: weight=0.9999999905966097:loss=1.0000000511642626\n",
            "iteration 731: weight=0.9999999908456556:loss=1.0000000498091925\n",
            "iteration 732: weight=0.9999999910881057:loss=1.000000048490011\n",
            "iteration 733: weight=0.9999999913241344:loss=1.0000000472057677\n",
            "iteration 734: weight=0.9999999915539122:loss=1.0000000459555372\n",
            "iteration 735: weight=0.9999999917776043:loss=1.0000000447384185\n",
            "iteration 736: weight=0.9999999919953719:loss=1.000000043553535\n",
            "iteration 737: weight=0.9999999922073721:loss=1.0000000424000328\n",
            "iteration 738: weight=0.9999999924137575:loss=1.0000000412770806\n",
            "iteration 739: weight=0.9999999926146768:loss=1.0000000401838696\n",
            "iteration 740: weight=0.9999999928102747:loss=1.000000039119612\n",
            "iteration 741: weight=0.9999999930006924:loss=1.000000038083541\n",
            "iteration 742: weight=0.999999993186067:loss=1.0000000370749098\n",
            "iteration 743: weight=0.999999993366532:loss=1.0000000360929922\n",
            "iteration 744: weight=0.9999999935422175:loss=1.0000000351370801\n",
            "iteration 745: weight=0.99999999371325:loss=1.0000000342064852\n",
            "iteration 746: weight=0.9999999938797526:loss=1.0000000333005366\n",
            "iteration 747: weight=0.9999999940418456:loss=1.000000032418582\n",
            "iteration 748: weight=0.9999999941996456:loss=1.0000000315599855\n",
            "iteration 749: weight=0.9999999943532663:loss=1.0000000307241288\n",
            "iteration 750: weight=0.9999999945028184:loss=1.0000000299104095\n",
            "iteration 751: weight=0.9999999946484095:loss=1.0000000291182412\n",
            "iteration 752: weight=0.9999999947901448:loss=1.0000000283470531\n",
            "iteration 753: weight=0.9999999949281262:loss=1.00000002759629\n",
            "iteration 754: weight=0.9999999950624532:loss=1.0000000268654106\n",
            "iteration 755: weight=0.9999999951932226:loss=1.0000000261538882\n",
            "iteration 756: weight=0.9999999953205285:loss=1.0000000254612103\n",
            "iteration 757: weight=0.9999999954444629:loss=1.0000000247868779\n",
            "iteration 758: weight=0.9999999955651149:loss=1.000000024130405\n",
            "iteration 759: weight=0.9999999956825714:loss=1.0000000234913184\n",
            "iteration 760: weight=0.9999999957969172:loss=1.0000000228691581\n",
            "iteration 761: weight=0.9999999959082346:loss=1.0000000222634755\n",
            "iteration 762: weight=0.9999999960166037:loss=1.000000021673834\n",
            "iteration 763: weight=0.9999999961221027:loss=1.0000000210998092\n",
            "iteration 764: weight=0.9999999962248076:loss=1.0000000205409871\n",
            "iteration 765: weight=0.9999999963247925:loss=1.0000000199969654\n",
            "iteration 766: weight=0.9999999964221293:loss=1.000000019467352\n",
            "iteration 767: weight=0.9999999965168882:loss=1.000000018951765\n",
            "iteration 768: weight=0.9999999966091373:loss=1.0000000184498332\n",
            "iteration 769: weight=0.9999999966989432:loss=1.000000017961195\n",
            "iteration 770: weight=0.9999999967863706:loss=1.0000000174854982\n",
            "iteration 771: weight=0.9999999968714826:loss=1.0000000170224002\n",
            "iteration 772: weight=0.9999999969543405:loss=1.000000016571567\n",
            "iteration 773: weight=0.999999997035004:loss=1.000000016132674\n",
            "iteration 774: weight=0.9999999971135309:loss=1.0000000157054052\n",
            "iteration 775: weight=0.9999999971899782:loss=1.0000000152894524\n",
            "iteration 776: weight=0.9999999972644007:loss=1.0000000148845158\n",
            "iteration 777: weight=0.9999999973368523:loss=1.0000000144903038\n",
            "iteration 778: weight=0.999999997407385:loss=1.0000000141065326\n",
            "iteration 779: weight=0.9999999974760496:loss=1.0000000137329252\n",
            "iteration 780: weight=0.9999999975428957:loss=1.000000013369213\n",
            "iteration 781: weight=0.9999999976079714:loss=1.0000000130151334\n",
            "iteration 782: weight=0.9999999976713236:loss=1.0000000126704316\n",
            "iteration 783: weight=0.9999999977329979:loss=1.000000012334859\n",
            "iteration 784: weight=0.9999999977930387:loss=1.0000000120081738\n",
            "iteration 785: weight=0.9999999978514895:loss=1.000000011690141\n",
            "iteration 786: weight=0.9999999979083922:loss=1.0000000113805312\n",
            "iteration 787: weight=0.9999999979637879:loss=1.0000000110791212\n",
            "iteration 788: weight=0.9999999980177163:loss=1.000000010785694\n",
            "iteration 789: weight=0.9999999980702166:loss=1.000000010500038\n",
            "iteration 790: weight=0.9999999981213263:loss=1.0000000102219475\n",
            "iteration 791: weight=0.9999999981710825:loss=1.0000000099512223\n",
            "iteration 792: weight=0.9999999982195209:loss=1.000000009687667\n",
            "iteration 793: weight=0.9999999982666763:loss=1.000000009431092\n",
            "iteration 794: weight=0.9999999983125829:loss=1.0000000091813122\n",
            "iteration 795: weight=0.9999999983572736:loss=1.0000000089381478\n",
            "iteration 796: weight=0.9999999984007808:loss=1.0000000087014236\n",
            "iteration 797: weight=0.9999999984431356:loss=1.0000000084709688\n",
            "iteration 798: weight=0.9999999984843686:loss=1.0000000082466178\n",
            "iteration 799: weight=0.9999999985245096:loss=1.0000000080282085\n",
            "iteration 800: weight=0.9999999985635875:loss=1.000000007815584\n",
            "iteration 801: weight=0.9999999986016304:loss=1.0000000076085904\n",
            "iteration 802: weight=0.9999999986386658:loss=1.0000000074070794\n",
            "iteration 803: weight=0.9999999986747203:loss=1.000000007210905\n",
            "iteration 804: weight=0.99999999870982:loss=1.0000000070199264\n",
            "iteration 805: weight=0.99999999874399:loss=1.0000000068340058\n",
            "iteration 806: weight=0.999999998777255:loss=1.0000000066530093\n",
            "iteration 807: weight=0.9999999988096391:loss=1.0000000064768062\n",
            "iteration 808: weight=0.9999999988411654:loss=1.00000000630527\n",
            "iteration 809: weight=0.9999999988718569:loss=1.0000000061382768\n",
            "iteration 810: weight=0.9999999989017354:loss=1.0000000059757064\n",
            "iteration 811: weight=0.9999999989308226:loss=1.0000000058174416\n",
            "iteration 812: weight=0.9999999989591395:loss=1.0000000056633684\n",
            "iteration 813: weight=0.9999999989867063:loss=1.0000000055133758\n",
            "iteration 814: weight=0.9999999990135431:loss=1.0000000053673557\n",
            "iteration 815: weight=0.9999999990396691:loss=1.000000005225203\n",
            "iteration 816: weight=0.9999999990651032:loss=1.000000005086815\n",
            "iteration 817: weight=0.9999999990898636:loss=1.0000000049520923\n",
            "iteration 818: weight=0.9999999991139683:loss=1.0000000048209374\n",
            "iteration 819: weight=0.9999999991374346:loss=1.0000000046932565\n",
            "iteration 820: weight=0.9999999991602795:loss=1.000000004568957\n",
            "iteration 821: weight=0.9999999991825193:loss=1.0000000044479496\n",
            "iteration 822: weight=0.9999999992041699:loss=1.000000004330147\n",
            "iteration 823: weight=0.9999999992252473:loss=1.0000000042154642\n",
            "iteration 824: weight=0.9999999992457664:loss=1.0000000041038188\n",
            "iteration 825: weight=0.999999999265742:loss=1.0000000039951304\n",
            "iteration 826: weight=0.9999999992851887:loss=1.0000000038893204\n",
            "iteration 827: weight=0.9999999993041202:loss=1.000000003786313\n",
            "iteration 828: weight=0.9999999993225503:loss=1.0000000036860337\n",
            "iteration 829: weight=0.9999999993404924:loss=1.00000000358841\n",
            "iteration 830: weight=0.9999999993579594:loss=1.000000003493372\n",
            "iteration 831: weight=0.9999999993749635:loss=1.0000000034008512\n",
            "iteration 832: weight=0.9999999993915174:loss=1.0000000033107805\n",
            "iteration 833: weight=0.999999999407633:loss=1.0000000032230956\n",
            "iteration 834: weight=0.9999999994233215:loss=1.000000003137733\n",
            "iteration 835: weight=0.9999999994385946:loss=1.000000003054631\n",
            "iteration 836: weight=0.9999999994534632:loss=1.00000000297373\n",
            "iteration 837: weight=0.999999999467938:loss=1.0000000028949718\n",
            "iteration 838: weight=0.9999999994820294:loss=1.0000000028182994\n",
            "iteration 839: weight=0.9999999994957478:loss=1.0000000027436577\n",
            "iteration 840: weight=0.9999999995091027:loss=1.0000000026709928\n",
            "iteration 841: weight=0.9999999995221038:loss=1.0000000026002525\n",
            "iteration 842: weight=0.9999999995347608:loss=1.0000000025313855\n",
            "iteration 843: weight=0.9999999995470825:loss=1.0000000024643427\n",
            "iteration 844: weight=0.9999999995590778:loss=1.0000000023990754\n",
            "iteration 845: weight=0.9999999995707556:loss=1.0000000023355369\n",
            "iteration 846: weight=0.999999999582124:loss=1.0000000022736808\n",
            "iteration 847: weight=0.9999999995931914:loss=1.0000000022134632\n",
            "iteration 848: weight=0.9999999996039657:loss=1.0000000021548403\n",
            "iteration 849: weight=0.9999999996144546:loss=1.00000000209777\n",
            "iteration 850: weight=0.9999999996246658:loss=1.000000002042211\n",
            "iteration 851: weight=0.9999999996346065:loss=1.0000000019881237\n",
            "iteration 852: weight=0.9999999996442839:loss=1.0000000019354687\n",
            "iteration 853: weight=0.999999999653705:loss=1.0000000018842083\n",
            "iteration 854: weight=0.9999999996628766:loss=1.0000000018343054\n",
            "iteration 855: weight=0.9999999996718052:loss=1.0000000017857242\n",
            "iteration 856: weight=0.9999999996804974:loss=1.0000000017384298\n",
            "iteration 857: weight=0.9999999996889593:loss=1.0000000016923878\n",
            "iteration 858: weight=0.9999999996971971:loss=1.0000000016475654\n",
            "iteration 859: weight=0.9999999997052167:loss=1.00000000160393\n",
            "iteration 860: weight=0.999999999713024:loss=1.0000000015614503\n",
            "iteration 861: weight=0.9999999997206246:loss=1.0000000015200956\n",
            "iteration 862: weight=0.9999999997280238:loss=1.0000000014798363\n",
            "iteration 863: weight=0.9999999997352269:loss=1.0000000014406432\n",
            "iteration 864: weight=0.9999999997422393:loss=1.0000000014024881\n",
            "iteration 865: weight=0.9999999997490661:loss=1.0000000013653436\n",
            "iteration 866: weight=0.9999999997557121:loss=1.0000000013291828\n",
            "iteration 867: weight=0.999999999762182:loss=1.0000000012939796\n",
            "iteration 868: weight=0.9999999997684805:loss=1.0000000012597088\n",
            "iteration 869: weight=0.9999999997746123:loss=1.0000000012263457\n",
            "iteration 870: weight=0.9999999997805815:loss=1.0000000011938663\n",
            "iteration 871: weight=0.9999999997863926:loss=1.000000001162247\n",
            "iteration 872: weight=0.9999999997920499:loss=1.0000000011314654\n",
            "iteration 873: weight=0.9999999997975575:loss=1.000000001101499\n",
            "iteration 874: weight=0.9999999998029192:loss=1.000000001072326\n",
            "iteration 875: weight=0.9999999998081388:loss=1.0000000010439256\n",
            "iteration 876: weight=0.9999999998132203:loss=1.0000000010162775\n",
            "iteration 877: weight=0.999999999818167:loss=1.0000000009893617\n",
            "iteration 878: weight=0.9999999998229827:loss=1.0000000009631587\n",
            "iteration 879: weight=0.999999999827671:loss=1.0000000009376497\n",
            "iteration 880: weight=0.9999999998322351:loss=1.0000000009128165\n",
            "iteration 881: weight=0.9999999998366782:loss=1.0000000008886407\n",
            "iteration 882: weight=0.9999999998410036:loss=1.0000000008651053\n",
            "iteration 883: weight=0.9999999998452147:loss=1.0000000008421934\n",
            "iteration 884: weight=0.9999999998493141:loss=1.0000000008198882\n",
            "iteration 885: weight=0.9999999998533049:loss=1.0000000007981737\n",
            "iteration 886: weight=0.99999999985719:loss=1.0000000007770344\n",
            "iteration 887: weight=0.9999999998609723:loss=1.000000000756455\n",
            "iteration 888: weight=0.9999999998646545:loss=1.0000000007364205\n",
            "iteration 889: weight=0.9999999998682392:loss=1.0000000007169167\n",
            "iteration 890: weight=0.9999999998717288:loss=1.0000000006979293\n",
            "iteration 891: weight=0.9999999998751261:loss=1.000000000679445\n",
            "iteration 892: weight=0.9999999998784335:loss=1.00000000066145\n",
            "iteration 893: weight=0.9999999998816531:loss=1.0000000006439316\n",
            "iteration 894: weight=0.9999999998847875:loss=1.0000000006268772\n",
            "iteration 895: weight=0.9999999998878388:loss=1.0000000006102745\n",
            "iteration 896: weight=0.9999999998908093:loss=1.0000000005941114\n",
            "iteration 897: weight=0.9999999998937013:loss=1.0000000005783767\n",
            "iteration 898: weight=0.9999999998965166:loss=1.0000000005630585\n",
            "iteration 899: weight=0.9999999998992573:loss=1.000000000548146\n",
            "iteration 900: weight=0.9999999999019253:loss=1.0000000005336285\n",
            "iteration 901: weight=0.9999999999045228:loss=1.0000000005194956\n",
            "iteration 902: weight=0.9999999999070515:loss=1.0000000005057368\n",
            "iteration 903: weight=0.9999999999095133:loss=1.0000000004923426\n",
            "iteration 904: weight=0.9999999999119098:loss=1.000000000479303\n",
            "iteration 905: weight=0.9999999999142428:loss=1.0000000004666088\n",
            "iteration 906: weight=0.9999999999165141:loss=1.0000000004542509\n",
            "iteration 907: weight=0.9999999999187252:loss=1.00000000044222\n",
            "iteration 908: weight=0.9999999999208777:loss=1.0000000004305079\n",
            "iteration 909: weight=0.9999999999229732:loss=1.000000000419106\n",
            "iteration 910: weight=0.9999999999250131:loss=1.000000000408006\n",
            "iteration 911: weight=0.9999999999269991:loss=1.0000000003972003\n",
            "iteration 912: weight=0.9999999999289324:loss=1.0000000003866807\n",
            "iteration 913: weight=0.9999999999308147:loss=1.0000000003764395\n",
            "iteration 914: weight=0.999999999932647:loss=1.0000000003664697\n",
            "iteration 915: weight=0.9999999999344309:loss=1.000000000356764\n",
            "iteration 916: weight=0.9999999999361675:loss=1.0000000003473153\n",
            "iteration 917: weight=0.9999999999378582:loss=1.0000000003381166\n",
            "iteration 918: weight=0.999999999939504:loss=1.0000000003291618\n",
            "iteration 919: weight=0.9999999999411062:loss=1.0000000003204441\n",
            "iteration 920: weight=0.9999999999426661:loss=1.0000000003119571\n",
            "iteration 921: weight=0.9999999999441846:loss=1.000000000303695\n",
            "iteration 922: weight=0.9999999999456628:loss=1.0000000002956517\n",
            "iteration 923: weight=0.9999999999471019:loss=1.0000000002878213\n",
            "iteration 924: weight=0.999999999948503:loss=1.0000000002801985\n",
            "iteration 925: weight=0.9999999999498668:loss=1.0000000002727774\n",
            "iteration 926: weight=0.9999999999511946:loss=1.000000000265553\n",
            "iteration 927: weight=0.9999999999524871:loss=1.0000000002585199\n",
            "iteration 928: weight=0.9999999999537454:loss=1.0000000002516731\n",
            "iteration 929: weight=0.9999999999549705:loss=1.0000000002450076\n",
            "iteration 930: weight=0.9999999999561631:loss=1.0000000002385185\n",
            "iteration 931: weight=0.9999999999573241:loss=1.0000000002322016\n",
            "iteration 932: weight=0.9999999999584543:loss=1.0000000002260516\n",
            "iteration 933: weight=0.9999999999595546:loss=1.0000000002200649\n",
            "iteration 934: weight=0.9999999999606257:loss=1.0000000002142364\n",
            "iteration 935: weight=0.9999999999616684:loss=1.0000000002085625\n",
            "iteration 936: weight=0.9999999999626836:loss=1.000000000203039\n",
            "iteration 937: weight=0.999999999963672:loss=1.0000000001976617\n",
            "iteration 938: weight=0.9999999999646341:loss=1.0000000001924267\n",
            "iteration 939: weight=0.9999999999655707:loss=1.0000000001873304\n",
            "iteration 940: weight=0.9999999999664826:loss=1.000000000182369\n",
            "iteration 941: weight=0.9999999999673703:loss=1.000000000177539\n",
            "iteration 942: weight=0.9999999999682345:loss=1.000000000172837\n",
            "iteration 943: weight=0.9999999999690758:loss=1.0000000001682596\n",
            "iteration 944: weight=0.999999999969895:loss=1.0000000001638032\n",
            "iteration 945: weight=0.9999999999706923:loss=1.0000000001594649\n",
            "iteration 946: weight=0.9999999999714686:loss=1.0000000001552416\n",
            "iteration 947: weight=0.9999999999722242:loss=1.00000000015113\n",
            "iteration 948: weight=0.9999999999729599:loss=1.0000000001471272\n",
            "iteration 949: weight=0.999999999973676:loss=1.0000000001432305\n",
            "iteration 950: weight=0.9999999999743732:loss=1.0000000001394371\n",
            "iteration 951: weight=0.999999999975052:loss=1.000000000135744\n",
            "iteration 952: weight=0.9999999999757128:loss=1.000000000132149\n",
            "iteration 953: weight=0.999999999976356:loss=1.0000000001286489\n",
            "iteration 954: weight=0.9999999999769822:loss=1.0000000001252416\n",
            "iteration 955: weight=0.9999999999775917:loss=1.0000000001219247\n",
            "iteration 956: weight=0.9999999999781852:loss=1.0000000001186955\n",
            "iteration 957: weight=0.999999999978763:loss=1.0000000001155518\n",
            "iteration 958: weight=0.9999999999793254:loss=1.0000000001124913\n",
            "iteration 959: weight=0.999999999979873:loss=1.0000000001095122\n",
            "iteration 960: weight=0.9999999999804061:loss=1.0000000001066118\n",
            "iteration 961: weight=0.999999999980925:loss=1.000000000103788\n",
            "iteration 962: weight=0.9999999999814302:loss=1.0000000001010392\n",
            "iteration 963: weight=0.999999999981922:loss=1.0000000000983633\n",
            "iteration 964: weight=0.9999999999824007:loss=1.000000000095758\n",
            "iteration 965: weight=0.9999999999828668:loss=1.000000000093222\n",
            "iteration 966: weight=0.9999999999833207:loss=1.0000000000907532\n",
            "iteration 967: weight=0.9999999999837623:loss=1.0000000000883495\n",
            "iteration 968: weight=0.9999999999841924:loss=1.0000000000860096\n",
            "iteration 969: weight=0.999999999984611:loss=1.0000000000837317\n",
            "iteration 970: weight=0.9999999999850187:loss=1.0000000000815141\n",
            "iteration 971: weight=0.9999999999854154:loss=1.0000000000793552\n",
            "iteration 972: weight=0.9999999999858018:loss=1.0000000000772535\n",
            "iteration 973: weight=0.999999999986178:loss=1.0000000000752074\n",
            "iteration 974: weight=0.9999999999865441:loss=1.0000000000732157\n",
            "iteration 975: weight=0.9999999999869005:loss=1.0000000000712763\n",
            "iteration 976: weight=0.9999999999872473:loss=1.0000000000693885\n",
            "iteration 977: weight=0.999999999987585:loss=1.0000000000675509\n",
            "iteration 978: weight=0.9999999999879139:loss=1.0000000000657616\n",
            "iteration 979: weight=0.9999999999882341:loss=1.00000000006402\n",
            "iteration 980: weight=0.9999999999885456:loss=1.0000000000623244\n",
            "iteration 981: weight=0.9999999999888489:loss=1.0000000000606737\n",
            "iteration 982: weight=0.9999999999891442:loss=1.0000000000590668\n",
            "iteration 983: weight=0.9999999999894318:loss=1.0000000000575024\n",
            "iteration 984: weight=0.9999999999897118:loss=1.0000000000559794\n",
            "iteration 985: weight=0.9999999999899842:loss=1.0000000000544969\n",
            "iteration 986: weight=0.9999999999902496:loss=1.0000000000530533\n",
            "iteration 987: weight=0.9999999999905078:loss=1.0000000000516482\n",
            "iteration 988: weight=0.9999999999907592:loss=1.0000000000502802\n",
            "iteration 989: weight=0.9999999999910039:loss=1.0000000000489486\n",
            "iteration 990: weight=0.9999999999912421:loss=1.000000000047652\n",
            "iteration 991: weight=0.9999999999914742:loss=1.00000000004639\n",
            "iteration 992: weight=0.9999999999917:loss=1.0000000000451614\n",
            "iteration 993: weight=0.9999999999919198:loss=1.0000000000439653\n",
            "iteration 994: weight=0.9999999999921338:loss=1.0000000000428009\n",
            "iteration 995: weight=0.9999999999923421:loss=1.0000000000416671\n",
            "iteration 996: weight=0.9999999999925449:loss=1.0000000000405636\n",
            "iteration 997: weight=0.9999999999927423:loss=1.0000000000394893\n",
            "iteration 998: weight=0.9999999999929345:loss=1.0000000000384435\n",
            "iteration 999: weight=0.9999999999931217:loss=1.0000000000374254\n",
            "iteration 1000: weight=0.9999999999933038:loss=1.0000000000364342\n",
            "iteration 1001: weight=0.9999999999934812:loss=1.0000000000354692\n",
            "iteration 1002: weight=0.999999999993654:loss=1.0000000000345297\n",
            "iteration 1003: weight=0.999999999993822:loss=1.000000000033615\n",
            "iteration 1004: weight=0.9999999999939857:loss=1.000000000032725\n",
            "iteration 1005: weight=0.9999999999941449:loss=1.000000000031858\n",
            "iteration 1006: weight=0.9999999999942999:loss=1.0000000000310143\n",
            "iteration 1007: weight=0.9999999999944509:loss=1.000000000030193\n",
            "iteration 1008: weight=0.9999999999945979:loss=1.0000000000293934\n",
            "iteration 1009: weight=0.9999999999947409:loss=1.0000000000286149\n",
            "iteration 1010: weight=0.9999999999948801:loss=1.000000000027857\n",
            "iteration 1011: weight=0.9999999999950158:loss=1.0000000000271192\n",
            "iteration 1012: weight=0.9999999999951477:loss=1.000000000026401\n",
            "iteration 1013: weight=0.9999999999952762:loss=1.0000000000257019\n",
            "iteration 1014: weight=0.9999999999954012:loss=1.000000000025021\n",
            "iteration 1015: weight=0.9999999999955231:loss=1.0000000000243585\n",
            "iteration 1016: weight=0.9999999999956417:loss=1.0000000000237135\n",
            "iteration 1017: weight=0.9999999999957572:loss=1.0000000000230855\n",
            "iteration 1018: weight=0.9999999999958695:loss=1.000000000022474\n",
            "iteration 1019: weight=0.999999999995979:loss=1.0000000000218787\n",
            "iteration 1020: weight=0.9999999999960856:loss=1.0000000000212994\n",
            "iteration 1021: weight=0.9999999999961893:loss=1.0000000000207352\n",
            "iteration 1022: weight=0.9999999999962903:loss=1.0000000000201859\n",
            "iteration 1023: weight=0.9999999999963887:loss=1.0000000000196512\n",
            "iteration 1024: weight=0.9999999999964844:loss=1.0000000000191307\n",
            "iteration 1025: weight=0.9999999999965774:loss=1.0000000000186238\n",
            "iteration 1026: weight=0.999999999996668:loss=1.0000000000181304\n",
            "iteration 1027: weight=0.9999999999967562:loss=1.0000000000176503\n",
            "iteration 1028: weight=0.9999999999968421:loss=1.0000000000171827\n",
            "iteration 1029: weight=0.9999999999969258:loss=1.0000000000167277\n",
            "iteration 1030: weight=0.9999999999970073:loss=1.0000000000162845\n",
            "iteration 1031: weight=0.9999999999970866:loss=1.000000000015853\n",
            "iteration 1032: weight=0.9999999999971638:loss=1.0000000000154332\n",
            "iteration 1033: weight=0.9999999999972389:loss=1.0000000000150244\n",
            "iteration 1034: weight=0.9999999999973119:loss=1.0000000000146263\n",
            "iteration 1035: weight=0.9999999999973832:loss=1.000000000014239\n",
            "iteration 1036: weight=0.9999999999974525:loss=1.0000000000138618\n",
            "iteration 1037: weight=0.99999999999752:loss=1.0000000000134945\n",
            "iteration 1038: weight=0.9999999999975857:loss=1.0000000000131373\n",
            "iteration 1039: weight=0.9999999999976497:loss=1.000000000012789\n",
            "iteration 1040: weight=0.9999999999977118:loss=1.0000000000124505\n",
            "iteration 1041: weight=0.9999999999977724:loss=1.0000000000121205\n",
            "iteration 1042: weight=0.9999999999978315:loss=1.0000000000117997\n",
            "iteration 1043: weight=0.999999999997889:loss=1.000000000011487\n",
            "iteration 1044: weight=0.999999999997945:loss=1.0000000000111826\n",
            "iteration 1045: weight=0.9999999999979994:loss=1.0000000000108864\n",
            "iteration 1046: weight=0.9999999999980524:loss=1.000000000010598\n",
            "iteration 1047: weight=0.999999999998104:loss=1.0000000000103173\n",
            "iteration 1048: weight=0.9999999999981541:loss=1.000000000010044\n",
            "iteration 1049: weight=0.999999999998203:loss=1.000000000009778\n",
            "iteration 1050: weight=0.9999999999982505:loss=1.0000000000095188\n",
            "iteration 1051: weight=0.9999999999982969:loss=1.0000000000092668\n",
            "iteration 1052: weight=0.999999999998342:loss=1.0000000000090215\n",
            "iteration 1053: weight=0.999999999998386:loss=1.0000000000087825\n",
            "iteration 1054: weight=0.9999999999984288:loss=1.0000000000085498\n",
            "iteration 1055: weight=0.9999999999984703:loss=1.0000000000083233\n",
            "iteration 1056: weight=0.9999999999985107:loss=1.0000000000081029\n",
            "iteration 1057: weight=0.9999999999985503:loss=1.0000000000078884\n",
            "iteration 1058: weight=0.9999999999985887:loss=1.0000000000076794\n",
            "iteration 1059: weight=0.999999999998626:loss=1.000000000007476\n",
            "iteration 1060: weight=0.9999999999986624:loss=1.000000000007278\n",
            "iteration 1061: weight=0.9999999999986979:loss=1.0000000000070852\n",
            "iteration 1062: weight=0.9999999999987323:loss=1.0000000000068976\n",
            "iteration 1063: weight=0.9999999999987659:loss=1.0000000000067149\n",
            "iteration 1064: weight=0.9999999999987985:loss=1.000000000006537\n",
            "iteration 1065: weight=0.9999999999988303:loss=1.0000000000063638\n",
            "iteration 1066: weight=0.9999999999988614:loss=1.0000000000061953\n",
            "iteration 1067: weight=0.9999999999988916:loss=1.0000000000060314\n",
            "iteration 1068: weight=0.9999999999989209:loss=1.0000000000058715\n",
            "iteration 1069: weight=0.9999999999989495:loss=1.000000000005716\n",
            "iteration 1070: weight=0.9999999999989773:loss=1.0000000000055647\n",
            "iteration 1071: weight=0.9999999999990044:loss=1.0000000000054172\n",
            "iteration 1072: weight=0.9999999999990308:loss=1.0000000000052738\n",
            "iteration 1073: weight=0.9999999999990565:loss=1.0000000000051341\n",
            "iteration 1074: weight=0.9999999999990816:loss=1.000000000004998\n",
            "iteration 1075: weight=0.999999999999106:loss=1.0000000000048657\n",
            "iteration 1076: weight=0.9999999999991298:loss=1.0000000000047367\n",
            "iteration 1077: weight=0.9999999999991529:loss=1.000000000004611\n",
            "iteration 1078: weight=0.9999999999991753:loss=1.0000000000044889\n",
            "iteration 1079: weight=0.9999999999991971:loss=1.0000000000043698\n",
            "iteration 1080: weight=0.9999999999992184:loss=1.000000000004254\n",
            "iteration 1081: weight=0.999999999999239:loss=1.0000000000041411\n",
            "iteration 1082: weight=0.9999999999992593:loss=1.0000000000040314\n",
            "iteration 1083: weight=0.9999999999992788:loss=1.0000000000039246\n",
            "iteration 1084: weight=0.9999999999992979:loss=1.0000000000038207\n",
            "iteration 1085: weight=0.9999999999993165:loss=1.0000000000037195\n",
            "iteration 1086: weight=0.9999999999993348:loss=1.0000000000036209\n",
            "iteration 1087: weight=0.9999999999993523:loss=1.000000000003525\n",
            "iteration 1088: weight=0.9999999999993694:loss=1.0000000000034315\n",
            "iteration 1089: weight=0.999999999999386:loss=1.0000000000033407\n",
            "iteration 1090: weight=0.9999999999994023:loss=1.000000000003252\n",
            "iteration 1091: weight=0.999999999999418:loss=1.000000000003166\n",
            "iteration 1092: weight=0.9999999999994333:loss=1.0000000000030822\n",
            "iteration 1093: weight=0.9999999999994484:loss=1.0000000000030007\n",
            "iteration 1094: weight=0.9999999999994631:loss=1.0000000000029212\n",
            "iteration 1095: weight=0.9999999999994773:loss=1.0000000000028437\n",
            "iteration 1096: weight=0.9999999999994911:loss=1.0000000000027685\n",
            "iteration 1097: weight=0.9999999999995046:loss=1.0000000000026952\n",
            "iteration 1098: weight=0.9999999999995177:loss=1.000000000002624\n",
            "iteration 1099: weight=0.9999999999995306:loss=1.0000000000025544\n",
            "iteration 1100: weight=0.999999999999543:loss=1.0000000000024867\n",
            "iteration 1101: weight=0.9999999999995552:loss=1.0000000000024207\n",
            "iteration 1102: weight=0.999999999999567:loss=1.0000000000023566\n",
            "iteration 1103: weight=0.9999999999995786:loss=1.0000000000022942\n",
            "iteration 1104: weight=0.9999999999995897:loss=1.0000000000022333\n",
            "iteration 1105: weight=0.9999999999996005:loss=1.000000000002174\n",
            "iteration 1106: weight=0.9999999999996112:loss=1.0000000000021165\n",
            "iteration 1107: weight=0.9999999999996214:loss=1.0000000000020604\n",
            "iteration 1108: weight=0.9999999999996314:loss=1.0000000000020057\n",
            "iteration 1109: weight=0.9999999999996412:loss=1.0000000000019527\n",
            "iteration 1110: weight=0.9999999999996507:loss=1.000000000001901\n",
            "iteration 1111: weight=0.99999999999966:loss=1.0000000000018505\n",
            "iteration 1112: weight=0.9999999999996692:loss=1.0000000000018014\n",
            "iteration 1113: weight=0.9999999999996778:loss=1.0000000000017537\n",
            "iteration 1114: weight=0.9999999999996863:loss=1.000000000001707\n",
            "iteration 1115: weight=0.9999999999996945:loss=1.000000000001662\n",
            "iteration 1116: weight=0.9999999999997025:loss=1.000000000001618\n",
            "iteration 1117: weight=0.9999999999997102:loss=1.0000000000015752\n",
            "iteration 1118: weight=0.999999999999718:loss=1.0000000000015337\n",
            "iteration 1119: weight=0.9999999999997256:loss=1.000000000001493\n",
            "iteration 1120: weight=0.9999999999997329:loss=1.0000000000014535\n",
            "iteration 1121: weight=0.99999999999974:loss=1.000000000001415\n",
            "iteration 1122: weight=0.9999999999997469:loss=1.0000000000013776\n",
            "iteration 1123: weight=0.9999999999997535:loss=1.000000000001341\n",
            "iteration 1124: weight=0.99999999999976:loss=1.0000000000013054\n",
            "iteration 1125: weight=0.9999999999997664:loss=1.000000000001271\n",
            "iteration 1126: weight=0.9999999999997726:loss=1.0000000000012372\n",
            "iteration 1127: weight=0.9999999999997786:loss=1.0000000000012046\n",
            "iteration 1128: weight=0.9999999999997844:loss=1.0000000000011726\n",
            "iteration 1129: weight=0.9999999999997902:loss=1.0000000000011415\n",
            "iteration 1130: weight=0.9999999999997957:loss=1.0000000000011113\n",
            "iteration 1131: weight=0.999999999999801:loss=1.000000000001082\n",
            "iteration 1132: weight=0.9999999999998064:loss=1.0000000000010534\n",
            "iteration 1133: weight=0.9999999999998115:loss=1.0000000000010254\n",
            "iteration 1134: weight=0.9999999999998164:loss=1.0000000000009983\n",
            "iteration 1135: weight=0.9999999999998213:loss=1.000000000000972\n",
            "iteration 1136: weight=0.9999999999998259:loss=1.0000000000009461\n",
            "iteration 1137: weight=0.9999999999998306:loss=1.0000000000009213\n",
            "iteration 1138: weight=0.999999999999835:loss=1.0000000000008968\n",
            "iteration 1139: weight=0.9999999999998395:loss=1.000000000000873\n",
            "iteration 1140: weight=0.9999999999998437:loss=1.00000000000085\n",
            "iteration 1141: weight=0.9999999999998479:loss=1.0000000000008276\n",
            "iteration 1142: weight=0.9999999999998519:loss=1.0000000000008056\n",
            "iteration 1143: weight=0.9999999999998559:loss=1.0000000000007843\n",
            "iteration 1144: weight=0.9999999999998597:loss=1.0000000000007636\n",
            "iteration 1145: weight=0.9999999999998634:loss=1.0000000000007434\n",
            "iteration 1146: weight=0.999999999999867:loss=1.0000000000007236\n",
            "iteration 1147: weight=0.9999999999998705:loss=1.0000000000007045\n",
            "iteration 1148: weight=0.9999999999998739:loss=1.000000000000686\n",
            "iteration 1149: weight=0.9999999999998772:loss=1.0000000000006677\n",
            "iteration 1150: weight=0.9999999999998805:loss=1.00000000000065\n",
            "iteration 1151: weight=0.9999999999998836:loss=1.0000000000006328\n",
            "iteration 1152: weight=0.9999999999998868:loss=1.000000000000616\n",
            "iteration 1153: weight=0.9999999999998899:loss=1.0000000000005997\n",
            "iteration 1154: weight=0.9999999999998928:loss=1.0000000000005838\n",
            "iteration 1155: weight=0.9999999999998956:loss=1.0000000000005684\n",
            "iteration 1156: weight=0.9999999999998983:loss=1.0000000000005533\n",
            "iteration 1157: weight=0.999999999999901:loss=1.0000000000005387\n",
            "iteration 1158: weight=0.9999999999999036:loss=1.0000000000005245\n",
            "iteration 1159: weight=0.9999999999999061:loss=1.0000000000005105\n",
            "iteration 1160: weight=0.9999999999999085:loss=1.000000000000497\n",
            "iteration 1161: weight=0.999999999999911:loss=1.0000000000004838\n",
            "iteration 1162: weight=0.9999999999999134:loss=1.0000000000004712\n",
            "iteration 1163: weight=0.9999999999999156:loss=1.0000000000004587\n",
            "iteration 1164: weight=0.9999999999999178:loss=1.0000000000004465\n",
            "iteration 1165: weight=0.9999999999999201:loss=1.0000000000004348\n",
            "iteration 1166: weight=0.9999999999999223:loss=1.0000000000004232\n",
            "iteration 1167: weight=0.9999999999999243:loss=1.000000000000412\n",
            "iteration 1168: weight=0.9999999999999263:loss=1.000000000000401\n",
            "iteration 1169: weight=0.9999999999999283:loss=1.0000000000003906\n",
            "iteration 1170: weight=0.9999999999999303:loss=1.0000000000003801\n",
            "iteration 1171: weight=0.999999999999932:loss=1.00000000000037\n",
            "iteration 1172: weight=0.9999999999999338:loss=1.0000000000003602\n",
            "iteration 1173: weight=0.9999999999999356:loss=1.0000000000003506\n",
            "iteration 1174: weight=0.9999999999999374:loss=1.0000000000003413\n",
            "iteration 1175: weight=0.9999999999999389:loss=1.0000000000003322\n",
            "iteration 1176: weight=0.9999999999999405:loss=1.0000000000003235\n",
            "iteration 1177: weight=0.999999999999942:loss=1.0000000000003149\n",
            "iteration 1178: weight=0.9999999999999436:loss=1.0000000000003066\n",
            "iteration 1179: weight=0.9999999999999452:loss=1.0000000000002984\n",
            "iteration 1180: weight=0.9999999999999467:loss=1.0000000000002907\n",
            "iteration 1181: weight=0.999999999999948:loss=1.0000000000002829\n",
            "iteration 1182: weight=0.9999999999999494:loss=1.0000000000002753\n",
            "iteration 1183: weight=0.9999999999999507:loss=1.000000000000268\n",
            "iteration 1184: weight=0.999999999999952:loss=1.000000000000261\n",
            "iteration 1185: weight=0.9999999999999534:loss=1.000000000000254\n",
            "iteration 1186: weight=0.9999999999999547:loss=1.0000000000002474\n",
            "iteration 1187: weight=0.9999999999999558:loss=1.0000000000002407\n",
            "iteration 1188: weight=0.9999999999999569:loss=1.0000000000002343\n",
            "iteration 1189: weight=0.999999999999958:loss=1.000000000000228\n",
            "iteration 1190: weight=0.9999999999999591:loss=1.000000000000222\n",
            "iteration 1191: weight=0.9999999999999603:loss=1.0000000000002163\n",
            "iteration 1192: weight=0.9999999999999614:loss=1.0000000000002105\n",
            "iteration 1193: weight=0.9999999999999625:loss=1.000000000000205\n",
            "iteration 1194: weight=0.9999999999999634:loss=1.0000000000001994\n",
            "iteration 1195: weight=0.9999999999999643:loss=1.000000000000194\n",
            "iteration 1196: weight=0.9999999999999651:loss=1.000000000000189\n",
            "iteration 1197: weight=0.999999999999966:loss=1.000000000000184\n",
            "iteration 1198: weight=0.9999999999999669:loss=1.0000000000001792\n",
            "iteration 1199: weight=0.9999999999999678:loss=1.0000000000001745\n",
            "iteration 1200: weight=0.9999999999999687:loss=1.0000000000001699\n",
            "iteration 1201: weight=0.9999999999999696:loss=1.0000000000001654\n",
            "iteration 1202: weight=0.9999999999999705:loss=1.0000000000001612\n",
            "iteration 1203: weight=0.9999999999999714:loss=1.0000000000001568\n",
            "iteration 1204: weight=0.999999999999972:loss=1.0000000000001525\n",
            "iteration 1205: weight=0.9999999999999727:loss=1.0000000000001485\n",
            "iteration 1206: weight=0.9999999999999734:loss=1.0000000000001446\n",
            "iteration 1207: weight=0.999999999999974:loss=1.0000000000001408\n",
            "iteration 1208: weight=0.9999999999999747:loss=1.000000000000137\n",
            "iteration 1209: weight=0.9999999999999754:loss=1.0000000000001334\n",
            "iteration 1210: weight=0.999999999999976:loss=1.0000000000001301\n",
            "iteration 1211: weight=0.9999999999999767:loss=1.0000000000001266\n",
            "iteration 1212: weight=0.9999999999999774:loss=1.0000000000001232\n",
            "iteration 1213: weight=0.999999999999978:loss=1.00000000000012\n",
            "iteration 1214: weight=0.9999999999999787:loss=1.0000000000001168\n",
            "iteration 1215: weight=0.9999999999999793:loss=1.0000000000001137\n",
            "iteration 1216: weight=0.9999999999999798:loss=1.0000000000001106\n",
            "iteration 1217: weight=0.9999999999999802:loss=1.0000000000001075\n",
            "iteration 1218: weight=0.9999999999999807:loss=1.0000000000001048\n",
            "iteration 1219: weight=0.9999999999999811:loss=1.000000000000102\n",
            "iteration 1220: weight=0.9999999999999816:loss=1.0000000000000993\n",
            "iteration 1221: weight=0.999999999999982:loss=1.0000000000000968\n",
            "iteration 1222: weight=0.9999999999999825:loss=1.0000000000000944\n",
            "iteration 1223: weight=0.9999999999999829:loss=1.000000000000092\n",
            "iteration 1224: weight=0.9999999999999833:loss=1.0000000000000895\n",
            "iteration 1225: weight=0.9999999999999838:loss=1.0000000000000873\n",
            "iteration 1226: weight=0.9999999999999842:loss=1.000000000000085\n",
            "iteration 1227: weight=0.9999999999999847:loss=1.0000000000000828\n",
            "iteration 1228: weight=0.9999999999999851:loss=1.0000000000000806\n",
            "iteration 1229: weight=0.9999999999999856:loss=1.0000000000000786\n",
            "iteration 1230: weight=0.999999999999986:loss=1.0000000000000764\n",
            "iteration 1231: weight=0.9999999999999865:loss=1.0000000000000744\n",
            "iteration 1232: weight=0.9999999999999869:loss=1.0000000000000724\n",
            "iteration 1233: weight=0.9999999999999873:loss=1.0000000000000704\n",
            "iteration 1234: weight=0.9999999999999878:loss=1.0000000000000684\n",
            "iteration 1235: weight=0.999999999999988:loss=1.0000000000000664\n",
            "iteration 1236: weight=0.9999999999999882:loss=1.0000000000000646\n",
            "iteration 1237: weight=0.9999999999999885:loss=1.0000000000000628\n",
            "iteration 1238: weight=0.9999999999999887:loss=1.0000000000000613\n",
            "iteration 1239: weight=0.9999999999999889:loss=1.0000000000000595\n",
            "iteration 1240: weight=0.9999999999999891:loss=1.0000000000000582\n",
            "iteration 1241: weight=0.9999999999999893:loss=1.0000000000000566\n",
            "iteration 1242: weight=0.9999999999999896:loss=1.0000000000000553\n",
            "iteration 1243: weight=0.9999999999999898:loss=1.000000000000054\n",
            "iteration 1244: weight=0.99999999999999:loss=1.0000000000000526\n",
            "iteration 1245: weight=0.9999999999999902:loss=1.0000000000000513\n",
            "iteration 1246: weight=0.9999999999999905:loss=1.0000000000000502\n",
            "iteration 1247: weight=0.9999999999999907:loss=1.0000000000000488\n",
            "iteration 1248: weight=0.9999999999999909:loss=1.0000000000000477\n",
            "iteration 1249: weight=0.9999999999999911:loss=1.0000000000000466\n",
            "iteration 1250: weight=0.9999999999999913:loss=1.0000000000000455\n",
            "iteration 1251: weight=0.9999999999999916:loss=1.0000000000000444\n",
            "iteration 1252: weight=0.9999999999999918:loss=1.0000000000000433\n",
            "iteration 1253: weight=0.999999999999992:loss=1.0000000000000422\n",
            "iteration 1254: weight=0.9999999999999922:loss=1.0000000000000413\n",
            "iteration 1255: weight=0.9999999999999925:loss=1.0000000000000402\n",
            "iteration 1256: weight=0.9999999999999927:loss=1.0000000000000393\n",
            "iteration 1257: weight=0.9999999999999929:loss=1.0000000000000382\n",
            "iteration 1258: weight=0.9999999999999931:loss=1.0000000000000373\n",
            "iteration 1259: weight=0.9999999999999933:loss=1.0000000000000362\n",
            "iteration 1260: weight=0.9999999999999936:loss=1.0000000000000353\n",
            "iteration 1261: weight=0.9999999999999938:loss=1.0000000000000344\n",
            "iteration 1262: weight=0.999999999999994:loss=1.0000000000000335\n",
            "iteration 1263: weight=0.9999999999999942:loss=1.0000000000000324\n",
            "iteration 1264: weight=0.9999999999999944:loss=1.0000000000000315\n",
            "iteration 1265: weight=0.9999999999999947:loss=1.0000000000000306\n",
            "iteration 1266: weight=0.9999999999999949:loss=1.0000000000000298\n",
            "iteration 1267: weight=0.9999999999999951:loss=1.0000000000000286\n",
            "iteration 1268: weight=0.9999999999999953:loss=1.0000000000000278\n",
            "iteration 1269: weight=0.9999999999999956:loss=1.0000000000000269\n",
            "iteration 1270: weight=0.9999999999999958:loss=1.000000000000026\n",
            "iteration 1271: weight=0.999999999999996:loss=1.000000000000025\n",
            "iteration 1272: weight=0.9999999999999962:loss=1.0000000000000242\n",
            "iteration 1273: weight=0.9999999999999964:loss=1.0000000000000233\n",
            "iteration 1274: weight=0.9999999999999967:loss=1.0000000000000224\n",
            "iteration 1275: weight=0.9999999999999967:loss=1.0000000000000215\n",
            "converged after 1275 iterations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(k,l2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "j3vQ7Xj5zQeK",
        "outputId": "23125773-e819-4e10-a4f0-2fc391b94d74"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f93d11e3650>]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAbmklEQVR4nO3da5Bc9Xnn8e8zfZn7aCTNoCsbCRZjg4MNHleBWccx2Jh4KbNJeAG21xA7S5WzSZxsailYV60rL3YrF1c2ycYVR7YJ3l2WxMEkoVxJHGJ7w25McAYbY4EgCBmQZAnN6D4zmsvp8+yLc3rUGs21+/R0/7t/nyrVdJ8+0+eZM6Pf/Oc5/3OOuTsiIhKejkYXICIi1VGAi4gESgEuIhIoBbiISKAU4CIigcqv58aGhoZ8165d67lJEZHgPfPMM+PuPrxw+boG+K5duxgdHV3PTYqIBM/MXltsuVooIiKBUoCLiARKAS4iEigFuIhIoBTgIiKBUoCLiARKAS4iEqhgA/zbr4zz1CvHG12GiEjDrOuJPFmZjWI+/IWnAXj+1z9Ab2eQX4aISE2CHIHvPzYx/3jv4dMNrEREpHGCDPDXjk/OPz4wPrnMmiIirSvIAB+fmJl/fGBsYpk1RURaV5DN47GJWcxg91Avrx2fanQ5IiINseII3MweNLNjZrZ3wfJfMrMXzex5M/ut+pV4sfGJGTb1FNkx2M2xszMrf4KISAtaTQvlIeDWygVm9l7gduBt7n418NnsS1va2NkZhvo6Ge7vZEwBLiJtasUAd/cngRMLFn8S+A13n0nXOVaH2pZ0cnKWzX1FLunv4tjZadx9PTcvItIUqj2I+Sbg3Wb2tJn9vZm9c6kVzexeMxs1s9GxsbEqN3ehiZmIvs48WwY6mSs5J6fmMnlfEZGQVBvgeWATcD3wH4GvmJkttqK773H3EXcfGR6+6I5AVTk7HdHXlWeorxOA4xNqo4hI+6k2wA8Bj3niO0AMDGVX1vImZ5MR+MaeIoBG4CLSlqoN8L8A3gtgZm8CisB4VkUtx92ZmI7o7cwz2FMA4OTU7HpsWkSkqaw4D9zMHgF+Ehgys0PAZ4AHgQfTqYWzwN2+TkcSZ6KYKPZkBN6bjsAnFeAi0n5WDHB3v2uJlz6acS2rMjkTAaQtlPIIXC0UEWk/wZ1KP5EGeG9nnu5CjmK+g1NqoYhIGwo2wPs685gZG3sK6oGLSFsKLsAnZ0oA9HbmANjYU1QLRUTaUnABPj2XBHh3oSLAdRBTRNpQcAE+E8UAdObTAO9VC0VE2lOAAZ6MwDsLSekbugucnY4aWZKISEOEF+Bz5RF4Unp/V4Ez0+qBi0j7CS/A0xZKV9oDH+jKMz0Xz4/MRUTaRYABnrZQ0hH4QHdyMo/aKCLSboIL8Om5Cw9i9nclJ5MqwEWk3QQX4OUReLE8Au9KRuBnzqkPLiLtJcAAjynkjFxHcvnxcgtFBzJFpN2EF+BzMV1p+wTOj8DVQhGRdhNegEel+TngcL4HrhaKiLSb4AJ8ei6eP4AJaqGISPsKLsBnotL8FEKA3mKODoMz59RCEZH2smKAm9mDZnYsvfvOwtd+zczczNbtfpgzUTw/AyWtgYHuAmc1AheRNrOaEfhDwK0LF5rZpcAtwOsZ17SsmSiePwuzrL8rzxkdxBSRNrNigLv7k8CJRV76b8B9wLrcC7NsZu7CFgokM1F0EFNE2k1VPXAzux047O7fX8W695rZqJmNjo2NVbO5C0xHMZ0LRuADXboioYi0nzUHuJn1AP8J+M+rWd/d97j7iLuPDA8Pr3VzF1lsBJ60UDQCF5H2Us0I/HJgN/B9M3sV2Al818y2ZlnYUmaj+OIWSrdaKCLSfvJr/QR3/wFwSfl5GuIj7j6eYV1LWuwg5kBXQQcxRaTtrGYa4SPAU8CVZnbIzD5R/7KWtnAeOMBAd56JmYhSvK7HU0VEGmrFEbi737XC67syq2YVpucunAcOyV15ACamIzb0FNazHBGRhgnuTMzZ0oWn0kPF9VB0IFNE2khwAT5XSi4nW2n+muAKcBFpI0EFeCl23KGQW3gij+7KIyLtJ6gAnyslt1PLLxiB9+ua4CLShoIM8OLCEXi3rgkuIu0nsABPpgnmO5YagSvARaR9BBXgUToCLyxyKj2ohSIi7SWoAJ8tB3jHhWUXch10F3KahSIibSWoAI/SFkohbxe91t+V1whcRNpKUAE+Pwul4+KyFeAi0m4CC/B0BJ67uOyB7oJaKCLSVgIL8LQHnlushaIrEopIewkqwKO4HOBLtVA0AheR9hFUgM9G6TzwRUbgyX0xNQIXkfYRVICXR+ALz8SE5HooGoGLSDsJKsDPXwtl8RbKTBQzE5XWuywRkYZYzR15HjSzY2a2t2LZb5vZi2b2nJn9uZkN1rfMxPlZKIsfxASdjSki7WM1I/CHgFsXLHsCeKu7XwP8M/BAxnUt6vwslMWmEep0ehFpLysGuLs/CZxYsOxv3b2clP9Icmf6uouWmQfe36kLWolIe8miB/5x4K+XetHM7jWzUTMbHRsbq2lDs/NnYi5+Kj2gmSgi0jZqCnAz+zQQAQ8vtY6773H3EXcfGR4ermVz8yPwhTc1huRMTNAIXETax4p3pV+Kmd0D3Abc7O6eWUXLmFvFCFw9cBFpF1UFuJndCtwHvMfdp7ItaWlzS1wPHM7PQtH1UESkXaxmGuEjwFPAlWZ2yMw+AfwB0A88YWbPmtnn61wnUDGNcLGrEXbmMUPXQxGRtrHiCNzd71pk8ZfqUMuKomUuZtXRYfQVdTamiLSPIM/EzC3SA4ekD65ZKCLSLsIK8Ngp5jowWzzAB7oLGoGLSNsIK8CjeNErEZbprjwi0k6CCvAo9kXPwizr7ypwdkYjcBFpD0EF+GwpXvQAZtmAeuAi0kaCCvCoFK88AlcPXETaRFABPlfyVfXA1+nEUBGRhgoswJcfgQ90F4hi59ycbuogIq0vqACPSr7odVDKdD0UEWknYQV47OQXOY2+7PxdedQHF5HWF1SAl+Ll54EPpCPw05qJIiJtIKgAj2Jf8jR60AhcRNpLUAFeipfvgQ+oBy4ibSSoAF9pBF6+K4+uCS4i7SCoAC+teBBTI3ARaR9BBfhKI/DuQo5ch6kHLiJtIagAL8Xxsj1wM9P1UESkbazmlmoPmtkxM9tbsWyTmT1hZi+nHzfWt8xEVFp+BA66HoqItI/VjMAfAm5dsOx+4BvufgXwjfR53ZXi5a+FAromuIi0jxUD3N2fBE4sWHw78OX08ZeBf5NxXYsqxU5umYOYAANdBc1CEZG2UG0PfIu7H0kfHwW2LLWimd1rZqNmNjo2Nlbl5hJzK/TAQSNwEWkfNR/E9OTarUtev9Xd97j7iLuPDA8P17St0qp74ApwEWl91Qb4G2a2DSD9eCy7kpYWrXAmJsBAd54z59RCEZHWV22APw7cnT6+G/jLbMpZXmmFeeCQjMAnZiPiWDd1EJHWtppphI8ATwFXmtkhM/sE8BvA+83sZeB96fO6W9UIvCuPO0zMqo0iIq0tv9IK7n7XEi/dnHEtK1rtLBSAM+fm5h+LiLSioM7EjFa4Hjicv6DVafXBRaTFBRXgK11OFmCwJw3wKQW4iLS2oAJ8NT3wcoCfVICLSIsLJsDj2HFnxR74xp4iAKfOza5HWSIiDRNMgEfptMCVeuAb0h74KY3ARaTFBRPgpTTAV5oH3lXI0V3IcWpKI3ARaW3BBHgUxwAr9sAh6YOrBy4irS6YAF/tCBxgsKeoFoqItLxgAny+B76aAO8uqIUiIi0vmAA/PwJfueSNvQVO6UQeEWlxwQT4WkbgG7qLGoGLSMsLJsBLpbX0wAucmpojuVS5iEhrCibA52ehrDAPHGBjT4EodiZnS/UuS0SkYYIJ8DXNQulOzsY8Oak2ioi0rmACfE2zUHp0RUIRaX3BBPhaZqEMptdDOakDmSLSwmoKcDP7VTN73sz2mtkjZtaVVWELrWUEvrFH10MRkdZXdYCb2Q7gl4ERd38rkAPuzKqwhUrpQczV9MA3zAe4RuAi0rpqbaHkgW4zywM9wI9qL2lxUWktZ2Kml5TVCFxEWljVAe7uh4HPAq8DR4DT7v63C9czs3vNbNTMRsfGxqoudC2zUIr5DnqLOV3QSkRaWi0tlI3A7cBuYDvQa2YfXbieu+9x9xF3HxkeHq660NVeD7xsU1+RE5MzVW9PRKTZ1dJCeR/wQ3cfc/c54DHgXdmUdbG1zEIB2NzbyXHNAxeRFlZLgL8OXG9mPWZmwM3AvmzKuthaZqEADPUVGZ9QgItI66qlB/408CjwXeAH6Xvtyaiui6xlFgqkI/AJtVBEpHXla/lkd/8M8JmMalnWWkfgm/uKnJicJY6djlV+johISAI8E3O1Ad5JFDtnpjUTRURaUzABfn4e+OpKHupL5oKrDy4irSqYAJ8fga9yGuHm3k4A9cFFpGUFE+BrnoXSrxG4iLS2YAK8mlkoAMd1Mo+ItKhgAnytI/CNPQXMNAIXkdYVTICvdRZKPtfBxp6ieuAi0rKCCfDzI/DVl7y5t8hxjcBFpEUFE+BrHYFDcjKPeuAi0qqCCfC1XA+8bHNfp0bgItKyggnwUhxjxppOix/qLTKuHriItKhgAjyKfU2jb0hG4GemI6bnSnWqSkSkcYIJ8FLsa+p/A2wZSOaCj53VKFxEWk8wAZ6MwNdW7paBLgDeODNdj5JERBoqmACvZgS+dUMS4EcV4CLSgoIJ8CiO19wD39JfHoGrhSIiraemADezQTN71MxeNLN9ZnZDVoUtVM0IfLCnQDHfoRaKiLSkmu7IA/we8DfufoeZFYGeDGpaVFRa+ywUM2PLQKcCXERaUtUBbmYbgJ8A7gFw91mgbmfNlGJf9bXAK20d6OLoaQW4iLSeWloou4Ex4I/N7Htm9kUz682orovMVTELBeCSgS6OaRqhiLSgWgI8D1wH/KG7XwtMAvcvXMnM7jWzUTMbHRsbq3pjpThecw8czo/A3b3qbYuINKNaAvwQcMjdn06fP0oS6Bdw9z3uPuLuI8PDw1VvrJoeOCQn85ybK3F2Jqp62yIizajqAHf3o8BBM7syXXQz8EImVS2imlkoUHEyj/rgItJiap2F8kvAw+kMlAPAz9Ve0uKquRYKwPbBbgAOnzrHFVv6sy5LRKRhagpwd38WGMmolmVVOwLfufF8gIuItJKwzsTMVTELpb+LfIdx6KQCXERaSzABXqqyhZLrMLYPdivARaTlBBPgUZUtFEjaKIdPTmVckYhIYwUT4NWOwCEJcI3ARaTVBBPgUcnJVXEmJsCOwR6OnZ1hJtKdeUSkdQQT4LWOwAF+dEpzwUWkdQQT4FEcV3UxKzgf4IfUBxeRFhJMgNc0At+UXOX24An1wUWkdQQT4FHs5Ky6AN860EUx38FrxyczrkpEpHGCCfBS7OSrbKHkOoxdm3s4MK4AF5HWEUyAJ/PAqy9391AvP1SAi0gLCSbAa+mBA+we6uO145OUYl0XXERaQzABHpWqu6FD2WVDvcyVnMM6oUdEWkQ4AV7rCHw4udvbgfGJrEoSEWmooAK82nngkPTAAfXBRaRlBBPgpdgp1HAQc3Nvkf6uPAfGFOAi0hqCCHB3r/qGDmVmxpu29PPSG2czrExEpHFqDnAzy5nZ98zsa1kUtJjyzJFaeuAAb97az4tHzugO9SLSErIYgX8K2JfB+ywpSgO8lh44JAF+ZjriiG5wLCItoKYAN7OdwL8GvphNOYvLbAS+bQCAl46qjSIi4at1BP67wH1AvNQKZnavmY2a2ejY2FhVG4lK6Qi8hoOYAFduTe5Kv+/omZreR0SkGVSdiGZ2G3DM3Z9Zbj133+PuI+4+Mjw8XNW2ojj5/VDrCHygq8COwW5ePKIRuIiEr5Yh7Y3Ah8zsVeBPgJvM7H9lUtUC8y2UGnvgAG/Z1s/zPzpd8/uIiDRa1QHu7g+4+0533wXcCXzT3T+aWWUVoox64ADX7BzkwPgkZ6bnan4vEZFGCmIeeHkEXmsPHODtlw7iDs8d1ChcRMKWSYC7+/9x99uyeK/FZDkCf9ulgwA8e/Bkze8lItJIgYzAk4OYtZyJWbahu8Blw708qxG4iAQuiACfK2U3AoekjfLswVM6I1NEghZEgJ/vgWcT4O/ctYnxiRndYk1EghZEgJd74IVcNuXecNlmAL79yvFM3k9EpBGCCPAse+AAP7a5h+0bunjqlfFM3k9EpBGCCPAo4x64mXHD5UM89cpxYt0jU0QCFUSAZ90DB3jX5Zs5OTXHC0d0XRQRCVMQAR5leCp92XuuHMYMnnjhjczeU0RkPQUS4OUeeHblDvV18o5/sVEBLiLBCiPAM+6Bl73/qi28cOQMh05OZfq+IiLrIYgAz/JqhJVuuXorAH+z92im7ysish6CCPAsr4VSafdQL2/buYGvfvdwpu8rIrIeggjwLK9GuNDPXLeTfUfO8MKPNBtFRMISRIDXawQO8KG3baeQM74yejDz9xYRqacgAjzrMzErbewtcts12/mz0YOcPqebPIhIOIII8KyvRrjQz797N5OzJR75zut1eX8RkXqo5abGl5rZt8zsBTN73sw+lWVhlepxJmalq7dv4F2Xb+ahf3iV6blSXbYhIpK1WkbgEfBr7n4VcD3w783sqmzKWrCh+WmE9fuD4Rdv+pccPTPNl7/9at22ISKSpVpuanzE3b+bPj4L7AN2ZFVYpXIPvF4tFIB3XT7ETW++hD/41n5OTs7WbTsiIlnJZEhrZruAa4GnF3ntXjMbNbPRsbGxqt4/qnMLpeyBn3ozU7Ml/stf7avrdkREslBzgJtZH/BV4Ffc/aLJ1O6+x91H3H1keHi4qm2U6nwQs+yKLf188j2X8+gzh/g7XSNFRJpcTQFuZgWS8H7Y3R/LpqSLrdcIHOCXb76Ct2wb4L6vPsfBE7pGiog0r1pmoRjwJWCfu/9OdiVdLIpjch1Gssn6KuY7+NyHryUqxfy7/zHK2WnNDReR5lTLCPxG4N8CN5nZs+m/D2ZU1wWi2Ndl9F122XAfn/vIdew/NsHHHvyOTvARkaZUyyyU/+fu5u7XuPvb039/lWVxZaWSU1jHAAd49xXDfO4j17H38Gk+/IV/5PCpc+u6fRGRlQRxJuZ6j8DLPnD1VvZ8bITXjk9x2+//X775og5sikjzCCLAS7HX9SSe5bz3ykt4/Bdv5JL+Lj7+0Ci/8PAzGo2LSFMw9/W7K/vIyIiPjo6u+fN+cOg0h0+d49a3bq1DVaszE5X4wpMH+O/f3E/szs9et5N7btzFm7cONKwmEWkPZvaMu49ctDyEAG8mh0+dY8/fv8Ij/3SQ2Sjm6u0D3HbNdt7zpmHesq1/XWbKiEh7UYBn7MTkLI8/e5jHvneY5w6dBpIbJb/90g28dccGfnzHBi4b7mPHYDfFfBCdKhFpUgrwOjp2ZponXx7nH/aP89yhUxwYn6S8WzsMtm3oZsdgN5v7imzqLbK5r5PNvUX6u/L0FHP0FPP0duboLiQfO/M58jmj0NFBPmfzjzsacCBXRBpvqQDPN6KYVnPJQBd3vGMnd7xjJwATMxEvHjnDa8eneO3EFAdPTHH41DlePjbBiclZTk7NUs3vzQ5LrshY6DByHUZHh2GAWfkjgNFhyWPD0o/pOkstz2xPVKjT75p6vG292l76dSuV/uvP/Djv3LUp0/dUgNdBX2eekV2bGFnim1WKnZNTs0zOREzOlJiajZiaTT5OzpSYiWKiOGau5ESlmCh25koxUcmZi5OPpdhxd2IHx3EHh/QXgxPHFy+ff+5esSx79fqrri7vWqc/QL1ebyzB6i7kMn9PBXgD5DqMob5Ohvo6G12KiARMR9dERAKlABcRCZQCXEQkUApwEZFAKcBFRAKlABcRCZQCXEQkUApwEZFAreu1UMxsDHityk8fAsYzLGe9hVx/yLWD6m801V+7H3P34YUL1zXAa2Fmo4tdzCUUIdcfcu2g+htN9dePWigiIoFSgIuIBCqkAN/T6AJqFHL9IdcOqr/RVH+dBNMDFxGRC4U0AhcRkQoKcBGRQDV9gJvZrWb2kpntN7P7G13PYszsUjP7lpm9YGbPm9mn0uWbzOwJM3s5/bgxXW5m9vvp1/ScmV3X2K8gYWY5M/uemX0tfb7bzJ5O6/xTMyumyzvT5/vT13c1su60pkEze9TMXjSzfWZ2Qyj738x+Nf252Wtmj5hZVzPvezN70MyOmdneimVr3tdmdne6/stmdneD6//t9GfnOTP7czMbrHjtgbT+l8zsAxXLG59N7t60/4Ac8ApwGVAEvg9c1ei6FqlzG3Bd+rgf+GfgKuC3gPvT5fcDv5k+/iDw1yS3TbweeLrRX0Na138A/jfwtfT5V4A708efBz6ZPv4F4PPp4zuBP22C2r8M/Hz6uAgMhrD/gR3AD4Huin1+TzPve+AngOuAvRXL1rSvgU3AgfTjxvTxxgbWfwuQTx//ZkX9V6W50wnsTvMo1yzZ1JAf2jXs6BuAr1c8fwB4oNF1raLuvwTeD7wEbEuXbQNeSh//EXBXxfrz6zWw5p3AN4CbgK+l/+HGK36o578XwNeBG9LH+XQ9a2DtG9IQtAXLm37/pwF+MA2yfLrvP9Ds+x7YtSAA17SvgbuAP6pYfsF6613/gtd+Gng4fXxB5pT3f7NkU7O3UMo/3GWH0mVNK/2T9lrgaWCLux9JXzoKbEkfN+PX9bvAfUCcPt8MnHL3KH1eWeN8/enrp9P1G2U3MAb8cdoC+qKZ9RLA/nf3w8BngdeBIyT78hnC2fdla93XTfM9WMTHSf5qgCavv9kDPChm1gd8FfgVdz9T+Zonv6abcs6mmd0GHHP3ZxpdS5XyJH8S/6G7XwtMkvwZP69Z93/aK76d5JfQdqAXuLWhRdWoWff1apjZp4EIeLjRtaxGswf4YeDSiuc702VNx8wKJOH9sLs/li5+w8y2pa9vA46ly5vt67oR+JCZvQr8CUkb5feAQTPLp+tU1jhff/r6BuD4eha8wCHgkLs/nT5/lCTQQ9j/7wN+6O5j7j4HPEby/Qhl35etdV830/cAADO7B7gN+Ej6SwiavP5mD/B/Aq5Ij8gXSQ7aPN7gmi5iZgZ8Cdjn7r9T8dLjQPno+t0kvfHy8o+lR+ivB05X/Pm57tz9AXff6e67SPbxN939I8C3gDvS1RbWX/667kjXb9iIy92PAgfN7Mp00c3AC4Sx/18HrjeznvTnqFx7EPu+wlr39deBW8xsY/pXyC3psoYws1tJWogfcvepipceB+5MZ//sBq4AvkOzZNN6N92rONjwQZJZHa8An250PUvU+K9I/mR8Dng2/fdBkt7kN4CXgb8DNqXrG/C59Gv6ATDS6K+h4mv5Sc7PQrmM5Id1P/BnQGe6vCt9vj99/bImqPvtwGj6PfgLkpkNQex/4NeBF4G9wP8kmfHQtPseeISkXz9H8tfPJ6rZ1yS95v3pv59rcP37SXra5f+/n69Y/9Np/S8BP1WxvOHZpFPpRUQC1ewtFBERWYICXEQkUApwEZFAKcBFRAKlABcRCZQCXEQkUApwEZFA/X+3FIg3JQZguwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}